{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7K9I5HAQ99w"
      },
      "source": [
        "# IMPORT PACKAGES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwnEtmmXePTR"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "import scipy\n",
        "from scipy.linalg import logm,expm\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#for logistic regression\n",
        "from sklearn import linear_model\n",
        "from scipy.special import expit\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# import statistics\n",
        "# from scipy import stats\n",
        "\n",
        "# import itertools\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from IPython.core.debugger import set_trace         #for debugging \n",
        "\n",
        "np.random.seed(123) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URTIOev517Wa"
      },
      "source": [
        "# Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvQO4DmB2edH"
      },
      "source": [
        "##**Multinomial Naive Bayes Class**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwH6tIFFRGaJ"
      },
      "source": [
        "class MultinomialNaiveBayes:\n",
        "    \n",
        "    def __init__(self):\n",
        "        return\n",
        "    \n",
        "    def fit(self, x, y, alpha):\n",
        "        N, D = x.shape\n",
        "        C = np.max(y) + 1\n",
        "        Nc = np.zeros(C)                              # number of instances in class c\n",
        "        count_word= np.zeros((C,D))\n",
        "\n",
        "        for c in range(C):\n",
        "            x_c = x[y == c]                           # slice all the elements from class c\n",
        "            Nc[c] = x_c.shape[0]                      # get number of elements of class c [N(y=c)]\n",
        "            count_word[c,:] = np.sum(x_c,axis=0)      # count number of times the word appears in class C\n",
        "\n",
        "\n",
        "        self.pi = (Nc+alpha)/(N+C)                        # Laplace smoothing (using alpha_c=1 for all c) you can derive using Dirichlet's distribution\n",
        "        #self.pi = csr_matrix(self.pi)     #Turn pi to sparse matrix\n",
        "        self.theta_one = count_word+alpha     \n",
        "        #self.theta_one = csr_matrix(self.theta_one) #\n",
        "        self.theta_two = alpha*D+C\n",
        "        #self.theta_two = csr_matrix(self.theta_two) #\n",
        "        #self.theta = (count_word+1)/(D+C)\n",
        "        #print(self.theta_two.todense())\n",
        "        #print(np.log(self.theta_one[:,None,:])- np.log(self.theta_two))\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSyCYj0ARKL9"
      },
      "source": [
        "def logsumexp(Z):                                                # dimension C x N\n",
        "    Zmax = np.max(Z,axis=0)[None,:]                              # max over C\n",
        "    log_sum_exp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=0))\n",
        "    return log_sum_exp\n",
        "\n",
        "def predict(self, xt):\n",
        "    Nt, D = xt.shape\n",
        "    # for numerical stability we work in the log domain\n",
        "    # we add a dimension because this is added to the log-likelihood matrix \n",
        "    # that assigns a likelihood for each class (C) to each test point, and so it is C x N\n",
        "    log_prior = np.log(self.pi)[:, None]\n",
        "\n",
        "    # logarithm of the likelihood term for Multinomial \n",
        "    ##### log_likelihood = xt[None,:,:]*(np.log(self.theta_one[:,None,:]) - np.log(self.theta_two)) # C x N x D\n",
        "    C,D_theta = self.theta_one.shape\n",
        "    xx_test = csr_matrix(xt)\n",
        "    log_likelihood = np.zeros((Nt,C))\n",
        "    log_theta = np.log(self.theta_one) - np.log(self.theta_two)\n",
        "    for i in range(C):\n",
        "        mult = xt.multiply(csr_matrix(log_theta[i]))\n",
        "        mult = mult.sum(axis=1)\n",
        "        log_likelihood[:,i] = mult.reshape(mult.shape[0],)[0] \n",
        "        del mult\n",
        "        \n",
        "    log_likelihood = log_likelihood.T\n",
        "\n",
        "    # now we sum over the feature dimension to get a C x N matrix (this has the log-likelihood for each class-test point combination)\n",
        "    #log_likelihood = np.sum(log_likelihood, axis=2)\n",
        "\n",
        "    # posterior calculation\n",
        "    log_posterior = log_prior + log_likelihood\n",
        "    posterior = np.exp(log_posterior - logsumexp(log_posterior))\n",
        "    \n",
        "    return posterior.T                                               # dimension N x C\n",
        "\n",
        "MultinomialNaiveBayes.predict = predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjNE1qhX3n6D"
      },
      "source": [
        "#Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziefVD4dFv_e"
      },
      "source": [
        "# Fit the classifier\n",
        "clf = linear_model.LogisticRegression()\n",
        "\"\"\"clf.fit(X_newsgroup_train_tfidf, Y_train)\n",
        "\n",
        "predictions = clf.predict(X_newsgroup_test_tfidf)\n",
        "print(X_newsgroup_test_tfidf.shape, Y_test.shape)\n",
        "score = clf.score(X_newsgroup_test_tfidf, Y_test)  \n",
        "\n",
        "print(score)\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWRxrMv-taJO"
      },
      "source": [
        "#Data Pre-Process & Split Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDF4ALNifpB7"
      },
      "source": [
        "def cross_validation_split(data): # according to the assignment all that we take is the data as entry\n",
        "  imdb_vectorizer = CountVectorizer()\n",
        "  X_all_counts = imdb_vectorizer.fit_transform(data.data)\n",
        "  tfidf_transformer = TfidfTransformer()\n",
        "  X_train = X_all_counts[:15000,:]  # change the train-test split here\n",
        "  X_test = X_all_counts[15000:,:]  # change the train-test split here\n",
        "  X_train_tfidf = tfidf_transformer.fit_transform(X_train)\n",
        "  X_test_tfidf = tfidf_transformer.fit_transform(X_test)\n",
        "  Y_train = data.target[:15000] # change the train-test split here\n",
        "  Y_test = data.target[15000:] # change the train-test split here\n",
        "  num_folds = 10\n",
        "  (num_instances, num_features), num_classes = X_train.shape, np.max(Y_train)+1\n",
        "\n",
        "  n_test = num_instances\n",
        "  n_valid = len(data.data) - num_instances \n",
        "  inds_1 = np.random.permutation(n_test)\n",
        "  inds_2 = np.random.permutation(n_valid)\n",
        "\n",
        "  x_test, y_test = X_test_tfidf[inds_2[:n_valid],:], Y_test[inds_2[:n_valid]]\n",
        "  x_rest, y_rest = X_train_tfidf[inds_1[:n_test],:], Y_train[inds_1[:n_test]]\n",
        "  return x_test, y_test, x_rest, y_rest\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x-kAqzAfD_7"
      },
      "source": [
        "# Cross-Validation Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbdE0B_J6Og5"
      },
      "source": [
        "def evaluate_acc(pred, label):\n",
        "  accuracy = np.sum(y_pred == y_rest)/y_pred.shape[0]\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVTaAObUfxWI"
      },
      "source": [
        "def cross_validate(n, n_folds=5):\n",
        "    #get the number of data samples in each split\n",
        "    n_val = n // n_folds\n",
        "    inds = np.random.permutation(n)\n",
        "    inds = []\n",
        "    for f in range(n_folds):\n",
        "        tr_inds = []\n",
        "        #get the validation indexes\n",
        "        val_inds = list(range(f * n_val, (f+1)*n_val))\n",
        "        #get the train indexes\n",
        "        if f > 0:\n",
        "            tr_inds = list(range(f*n_val))\n",
        "        if f < n_folds - 1:\n",
        "            tr_inds = tr_inds + list(range((f+1)*n_val, n))\n",
        "        #The yield statement suspends functionâ€™s execution and sends a value back to the caller\n",
        "        #but retains enough state information to enable function to resume where it is left off\n",
        "        yield tr_inds, val_inds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQDIVvUYf2q6"
      },
      "source": [
        "\"\"\"Before using, make sure you have switched the is_naive_bayes boolean on or off. Also make sure you have\n",
        "chosen the correct test split in the cross-validation split data function\"\"\"\n",
        "\n",
        "def kfoldCV(x_test, y_test, x_rest, y_rest, model):\n",
        "  is_naive_bayes = True #switch to false if the model is logistic regression\n",
        "  num_folds = 5\n",
        "  K_list = [0.0001, 0.001, 0.01,0.1,0.5,1]\n",
        "  num_instances = x_rest.shape[0]\n",
        "  err_test, err_valid = np.zeros(len(K_list)), np.zeros((len(K_list), num_folds))\n",
        "  for i, K in enumerate(K_list):\n",
        "      #Find the validation errors for num_folds splits for a given K\n",
        "      for f, (tr, val) in enumerate(cross_validate(num_instances, num_folds)):\n",
        "        md = model\n",
        "        md = md.fit(x_rest[tr],y_rest[tr],K)\n",
        "        y_prob = model.predict(x_rest[val])\n",
        "        if is_naive_bayes:\n",
        "          y_pred = np.argmax(y_prob, 1)\n",
        "        else:\n",
        "          y_pred = y_prob\n",
        "        accuracy = np.sum(y_pred == y_rest[val])/y_pred.shape[0]\n",
        "        err_valid[i, f] = 1-accuracy\n",
        "\n",
        "      \n",
        "      #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
        "      md = model\n",
        "      md.fit(x_rest, y_rest,K)\n",
        "      y_prob = md.predict(x_test)\n",
        "      if is_naive_bayes:\n",
        "        y_pred = np.argmax(y_prob, 1)\n",
        "      else:\n",
        "        y_pred = y_prob\n",
        "      accuracy = np.sum(y_pred == y_test)/y_pred.shape[0]\n",
        "      #accuracy = evaluate_acc(y_pred, y_rest)\n",
        "      \n",
        "      err_test[i]= 1-accuracy\n",
        "      \n",
        "  plt.plot(K_list, err_test, label='test')\n",
        "  plt.errorbar(K_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
        "  plt.legend()\n",
        "  plt.xlabel('C')\n",
        "  plt.ylabel('error rate')\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHI2nIlXRGol"
      },
      "source": [
        "# Dataset 1: 20 newsgroup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2wXjVa56_WE"
      },
      "source": [
        "## Task 1: Acquiring and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-Ruh5kEGYPx"
      },
      "source": [
        "### IMPORT DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvpB06AtGd3g"
      },
      "source": [
        "newsgroup_all = fetch_20newsgroups(subset='all' ,remove=('headers','footers','quotes'))\r\n",
        "#newsgroup_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGZv788p7WgA"
      },
      "source": [
        "## Task 2: Implementing Cross-Valitdation on our classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUKSQYF5tIyo"
      },
      "source": [
        "### Naive Bayes Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiMMe3rB_x8u"
      },
      "source": [
        "x_test, y_test, x_rest, y_rest = cross_validation_split(newsgroup_all)\n",
        "#Remember to set the is_naive_bayes boolean to true or false! Remember to reset test train splits!\n",
        "kfoldCV(x_test, y_test, x_rest, y_rest, MultinomialNaiveBayes())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UWrrO0KFqhp"
      },
      "source": [
        "###Logistic Regression Cross Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqjwY0dU_RDr"
      },
      "source": [
        "x_test, y_test, x_rest, y_rest = cross_validation_split(newsgroup_all)\n",
        "#Remember to set the is_naive_bayes boolean to true or false! Remember to reset test train splits!\n",
        "kfoldCV(x_test, y_test, x_rest, y_rest, linear_model.LogisticRegression())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxqBWH7YunZW"
      },
      "source": [
        "###Train Best Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG4YmO_Ptv2L"
      },
      "source": [
        "best_md = MultinomialNaiveBayes()\r\n",
        "best_md = best_md.fit(x_rest,y_rest,0.01)\r\n",
        "\r\n",
        "y_prob = best_md.predict(x_test)\r\n",
        "y_pred = np.argmax(y_prob, 1)\r\n",
        "\r\n",
        "best_accuracy = np.sum(y_pred == y_test)/y_pred.shape[0]\r\n",
        "best_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHm0OCXpdk0R"
      },
      "source": [
        "## Task 3: Experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKgj65I2dnCH"
      },
      "source": [
        "### Find optimal hyperparameter for Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu_YOHdEdtf9"
      },
      "source": [
        "# get # train instances (from percent)\n",
        "all_vectorizer = CountVectorizer()\n",
        "X_newgroups_all_counts = all_vectorizer.fit_transform(newsgroup_all.data)\n",
        "num_train = round(50/100 * X_newgroups_all_counts.shape[0])\n",
        "\n",
        "# get data\n",
        "X_newgroups_train = X_newgroups_all_counts[:num_train,:]  # around 50% training\n",
        "X_newgroups_test = X_newgroups_all_counts[num_train:,:]   # around 50% testing\n",
        "news_tfidf_transformer = TfidfTransformer()\n",
        "X_newsgroup_train_tfidf = news_tfidf_transformer.fit_transform(X_newgroups_train)\n",
        "X_newsgroup_test_tfidf = news_tfidf_transformer.fit_transform(X_newgroups_test)\n",
        "Y_train = newsgroup_all.target[:num_train]\n",
        "Y_test = newsgroup_all.target[num_train:]\n",
        "\n",
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore') #ignores warnings\n",
        "    grid={\"penalty\": [\"l1\",\"l2\", \"elasticnet\", \"none\"], \"solver\": [\"sag\", \"saga\"], \"multi_class\": [\"multinomial\"]}# l1 lasso l2 ridge\n",
        "    logreg=LogisticRegression()\n",
        "    logreg_cv=GridSearchCV(logreg,grid,cv=5)\n",
        "    logreg_cv.fit(X_newsgroup_train_tfidf, Y_train)\n",
        "    print(\"tuned hyperparameters :(best parameters) \",logreg_cv.best_params_)\n",
        "    print(\"accuracy :\",logreg_cv.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6Cgsb7oeDXZ"
      },
      "source": [
        "### Different Train/Test splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jhldnum0TMh"
      },
      "source": [
        "#try diff splits\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score \n",
        "import warnings \n",
        "\n",
        "def evaluate_acc(pred, label):\n",
        "  accuracy = np.sum(pred == label)/y_pred.shape[0]\n",
        "  return accuracy\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "  warnings.simplefilter('ignore') #ignores warnings\n",
        "  train_perc = [20, 40, 60, 80, 90, 95]\n",
        "  NBlist = list()\n",
        "  Bernlist = list()\n",
        "  LRlist = list()\n",
        "  LRoptlist = list()\n",
        "  GDlist = list()\n",
        "  SVClist = list()\n",
        "  for perc in train_perc:\n",
        "    # get # train instances (from percent)\n",
        "    num_train = round(perc/100 * X_newgroups_all_counts.shape[0])\n",
        "\n",
        "    # get data\n",
        "    X_newgroups_train = X_newgroups_all_counts[:num_train,:]  # around 80% training\n",
        "    X_newgroups_test = X_newgroups_all_counts[num_train:,:]   # around 20% testing\n",
        "    news_tfidf_transformer = TfidfTransformer()\n",
        "    X_newsgroup_train_tfidf = news_tfidf_transformer.fit_transform(X_newgroups_train)\n",
        "    X_newsgroup_test_tfidf = news_tfidf_transformer.fit_transform(X_newgroups_test)\n",
        "    Y_train = newsgroup_all.target[:num_train]\n",
        "    Y_test = newsgroup_all.target[num_train:]\n",
        "\n",
        "    # NB\n",
        "    model = MultinomialNaiveBayes()\n",
        "    model.fit(X_newsgroup_train_tfidf,Y_train, 1)\n",
        "    y_prob = model.predict(X_newsgroup_test_tfidf)\n",
        "    y_pred = np.argmax(y_prob, 1)\n",
        "    acc =  evaluate_acc(y_pred, Y_test)\n",
        "    print('NB ', perc, '=', acc)\n",
        "    NBlist.append(acc)\n",
        "\n",
        "    # LR standard\n",
        "    lrmodel = LogisticRegression().fit(X_newsgroup_train_tfidf, Y_train)\n",
        "    y_pred = lrmodel.predict(X_newsgroup_test_tfidf)\n",
        "    acc =  evaluate_acc(y_pred, Y_test)\n",
        "    print('LR ', perc, '=', acc)\n",
        "    LRlist.append(acc)\n",
        "    \n",
        "    # LR\n",
        "    lroptmodel = LogisticRegression(multi_class= 'multinomial', penalty= 'none', solver= 'saga')\n",
        "    lroptmodel.fit(X_newsgroup_train_tfidf, Y_train)\n",
        "    y_pred = lroptmodel.predict(X_newsgroup_test_tfidf)\n",
        "    acc =  evaluate_acc(y_pred, Y_test)\n",
        "    print('LR opt', perc, '=', acc)\n",
        "    LRoptlist.append(acc)\n",
        "\n",
        "    #gradient descendt\n",
        "    gdmodel = SGDClassifier().fit(X_newsgroup_train_tfidf, Y_train)\n",
        "    y_pred = gdmodel.predict(X_newsgroup_test_tfidf)\n",
        "    acc =  evaluate_acc(y_pred, Y_test)\n",
        "    print('GD ', perc, '=', acc)\n",
        "    GDlist.append(acc)\n",
        "\n",
        "    #SVC\n",
        "    svcmodel = svm.LinearSVC().fit(X_newsgroup_train_tfidf, Y_train)\n",
        "    y_pred = svcmodel.predict(X_newsgroup_test_tfidf)\n",
        "    acc =  evaluate_acc(y_pred, Y_test)\n",
        "    print('SVC ', perc, '=', acc)\n",
        "    SVClist.append(acc)\n",
        "\n",
        "  plt.plot(train_perc, NBlist, 'r-', label='Multinomial Naive Bayes',alpha=0.7)\n",
        "  plt.plot(train_perc, LRlist, 'b-', label='Default Logistic Regression',alpha=0.7)\n",
        "  plt.plot(train_perc, LRoptlist, 'k-', label='Optimized Logistic Regression',alpha=0.7)\n",
        "  plt.plot(train_perc, GDlist, 'y-', label='Gradient Descent Classifier',alpha=0.7)\n",
        "  plt.plot(train_perc, SVClist, 'g-', label='SVC',alpha=0.7)\n",
        "  plt.xticks(np.arange(min(train_perc), max(train_perc)+1, 5.0))\n",
        "  plt.xlabel('Percent Training Data')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krnD_K9GRKkl"
      },
      "source": [
        "# Dataset 2: IMDB Reviews\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq7nhczLYaTE"
      },
      "source": [
        "## Task 1: Acquiring and preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZw28Cz77qPB"
      },
      "source": [
        "### Import Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EckLvj3CaD_5"
      },
      "source": [
        "Mounting drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x-U6714Z6ZG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-98jp6BVaE0K"
      },
      "source": [
        "Extract from Tar file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhykdsLfaIEe"
      },
      "source": [
        "import tarfile\n",
        "tf = tarfile.open(\"/content/drive/MyDrive/aclImdb_v1.tar\")\n",
        "tf.extractall()\n",
        "print('done extracting')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzVW8xN07H-I"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6DdrFs07HBC"
      },
      "source": [
        "testfolder = '/content/aclImdb/test/'\n",
        "trainfolder = '/content/aclImdb/train/'\n",
        "\n",
        "negData = list()\n",
        "posData = list()\n",
        "labels = list()\n",
        "\n",
        "# get train first\n",
        "# get pos data\n",
        "for filename in os.listdir(trainfolder+'pos'):\n",
        "  file = open(trainfolder+'pos/' + filename)\n",
        "  posData.append(file.read())\n",
        "  file.close()\n",
        "# get neg data\n",
        "for filename in os.listdir(trainfolder+'neg'):\n",
        "  file = open(trainfolder+'neg/' + filename)\n",
        "  negData.append(file.read())\n",
        "  file.close()\n",
        "\n",
        "trainData = negData + posData\n",
        "\n",
        "# neg = 0, pos = 1\n",
        "trainLabel = np.array([0]*len(negData) + [1]*len(posData))\n",
        "\n",
        "# get test second\n",
        "negData = list()\n",
        "posData = list()\n",
        "# get neg data\n",
        "for filename in os.listdir(testfolder+'pos'):\n",
        "  file = open(testfolder+'pos/' + filename)\n",
        "  posData.append(file.read())\n",
        "  file.close()\n",
        "# get pos data\n",
        "for filename in os.listdir(testfolder+'neg'):\n",
        "  file = open(testfolder+'neg/' + filename)\n",
        "  negData.append(file.read())\n",
        "  file.close()\n",
        "\n",
        "testData = negData + posData\n",
        "\n",
        "# neg = 0, pos = 1\n",
        "testLabel = np.array([0]*len(negData) + [1]*len(posData))\n",
        "\n",
        "allData = trainData+testData\n",
        "allLabel = np.concatenate((trainLabel, testLabel), axis=0)\n",
        "\n",
        "print(len(trainData), len(testData)) \n",
        "print(len(allData))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDSCj-eZpxcv"
      },
      "source": [
        "Create bundle (so matches format of dataset 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3Bn0C_cpzxF"
      },
      "source": [
        "class bundle:\n",
        "  def __init__(self, data, target, target_names):\n",
        "    self.data = data\n",
        "    self.target = target\n",
        "    self.target_names = target_names\n",
        "    self.labels = None\n",
        "\n",
        "#IMDB_train = bundle(trainData, trainLabel, ['neg', 'pos'])\n",
        "#IMDB_test = bundle(testData, testLabel, ['neg', 'pos'])\n",
        "\n",
        "IMDB_all = bundle(allData,allLabel,['neg','pos'])\n",
        "\n",
        "# have 4 attributes: data, target (0, 1), target_names (neg and pos), and predicted labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAQKfbA58E9_"
      },
      "source": [
        "## Task 2: Implementing Cross-Valitdation on our classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KcifwVzhrbv"
      },
      "source": [
        "###Naive Bayes Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG7Dz473tXgE"
      },
      "source": [
        "x_test, y_test, x_rest, y_rest = cross_validation_split(IMDB_all)\n",
        "#Remember to set the is_naive_bayes boolean to true or false! Remember to reset test train splits!\n",
        "kfoldCV(x_test, y_test, x_rest, y_rest, MultinomialNaiveBayes())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI-aykAqkO3h"
      },
      "source": [
        "### Logistic Regression Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_QJr89JYsGN"
      },
      "source": [
        "x_test, y_test, x_rest, y_rest = cross_validation_split(IMDB_all)\n",
        "kfoldCV(x_test, y_test, x_rest, y_rest, linear_model.LogisticRegression())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV9t6GuJuv60"
      },
      "source": [
        "**Train Best Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4AP0ZSEuyqg"
      },
      "source": [
        "best_md2 = MultinomialNaiveBayes()\r\n",
        "best_md2 = best_md2.fit(x_rest,y_rest,0.1)\r\n",
        "y_prob2 = best_md2.predict(x_test)\r\n",
        "y_pred2 = np.argmax(y_prob2, 1)\r\n",
        "\r\n",
        "accuracy2 = np.sum(y_pred2 == y_test)/y_pred2.shape[0]\r\n",
        "accuracy2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH1-GHm386Hm"
      },
      "source": [
        "## Task 3: Experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK6l_PQM_9j1"
      },
      "source": [
        "###Find optimal hyperparameter for Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPGSNZhs_8b9"
      },
      "source": [
        "import warnings\n",
        "\n",
        "N = X_allData.shape[0]\n",
        "num_train = round(50/100 * N) # 50-50 split\n",
        "\n",
        "inds = np.random.permutation(N) #random selection (otherwise, sometimes only get one class)\n",
        "\n",
        "X_IMDB_train = X_allData[inds[:num_train],:]  \n",
        "X_IMDB_test = X_allData[inds[num_train:],:]   \n",
        "IMDB_tfidf_transformer = TfidfTransformer()\n",
        "X_IMDB_train_tfidf = IMDB_tfidf_transformer.fit_transform(X_IMDB_train)\n",
        "X_IMDB_test_tfidf = IMDB_tfidf_transformer.fit_transform(X_IMDB_test)\n",
        "Y_train = IMDB_all.target[inds[:num_train]]\n",
        "Y_test = IMDB_all.target[inds[num_train:]]\n",
        "\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter('ignore') #ignores warnings\n",
        "    grid={\"penalty\":[\"l1\",\"l2\", \"elasticnet\", \"none\"], \"solver\": [\"sag\", \"saga\"], \"multi_class\": [\"bernoulli\", \"multinomial\"]}# l1 lasso l2 ridge\n",
        "    logreg=LogisticRegression()\n",
        "    logreg_cv=GridSearchCV(logreg,grid,cv=5)\n",
        "    logreg_cv.fit(X_IMDB_train_tfidf, Y_train)\n",
        "    print(\"tuned hyperparameters :(best parameters) \",logreg_cv.best_params_)\n",
        "    print(\"accuracy :\",logreg_cv.best_score_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDupesBU9Osh"
      },
      "source": [
        "###Different Train/Test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai89-_J49NEh"
      },
      "source": [
        "###Different Train/Test split\n",
        "#try diff splits\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "import warnings\n",
        "\n",
        "def evaluate_acc(pred, label):\n",
        "  accuracy = np.sum(pred == label)/y_pred.shape[0]\n",
        "  return accuracy\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "  warnings.simplefilter('ignore') #ignores warnings\n",
        "  train_perc = [20, 40, 60, 80, 90, 95]\n",
        "  NBlist = list()\n",
        "  Bernlist = list()\n",
        "  LRoptlist = list()\n",
        "  LRlist = list()\n",
        "  GDlist = list()\n",
        "  SVClist = list()\n",
        "  for perc in train_perc:\n",
        "    # get # train instances (from percent)\n",
        "    N = X_allData.shape[0]\n",
        "    num_train = round(perc/100 * N)\n",
        "\n",
        "    inds = np.random.permutation(N) #random selection (otherwise, sometimes only get one class)\n",
        "\n",
        "    X_IMDB_train = X_allData[inds[:num_train],:]  # around 80% training\n",
        "    X_IMDB_test = X_allData[inds[num_train:],:]   # around 20% testing\n",
        "    IMDB_tfidf_transformer = TfidfTransformer()\n",
        "    X_IMDB_train_tfidf = IMDB_tfidf_transformer.fit_transform(X_IMDB_train)\n",
        "    X_IMDB_test_tfidf = IMDB_tfidf_transformer.fit_transform(X_IMDB_test)\n",
        "    Y_train = IMDB_all.target[inds[:num_train]]\n",
        "    Y_test = IMDB_all.target[inds[num_train:]]\n",
        "\n",
        "    # NB\n",
        "    model = MultinomialNaiveBayes()\n",
        "    model.fit(X_IMDB_train_tfidf,Y_train, 1)\n",
        "    y_prob = model.predict(X_IMDB_test_tfidf)\n",
        "    y_pred = np.argmax(y_prob, 1)\n",
        "    acc =  evaluate_acc(y_pred, Y_test)\n",
        "    print('NB ', perc, '=', acc)\n",
        "    NBlist.append(acc)\n",
        "\n",
        "    # NB\n",
        "    bern = BernoulliNB()\n",
        "    bern.fit(X_IMDB_train_tfidf,Y_train)\n",
        "    y_pred = bern.predict(X_IMDB_test_tfidf)\n",
        "    acc =  evaluate_acc(y_pred, Y_test)\n",
        "    print('Bernoulli ', perc, '=', acc)\n",
        "    Bernlist.append(acc)\n",
        "\n",
        "    # LR standard\n",
        "    lrmodel = LogisticRegression().fit(X_IMDB_train_tfidf, Y_train)\n",
        "    y_pred = lrmodel.predict(X_IMDB_test_tfidf)\n",
        "    acc =  evaluate_acc(y_pred, Y_test)\n",
        "    print('LR ', perc, '=', acc)\n",
        "    LRlist.append(acc)\n",
        "\n",
        "    #LR optimized\n",
        "    lroptmodel = LogisticRegression(multi_class='multinomial', penalty='l2', solver='sag')\n",
        "    lroptmodel.fit(X_IMDB_train_tfidf, Y_train)\n",
        "    y_pred = lroptmodel.predict(X_IMDB_test_tfidf)\n",
        "    acc =  evaluate_acc(y_pred, Y_test)\n",
        "    print('LR opt', perc, '=', acc)\n",
        "    LRoptlist.append(acc)\n",
        "    \n",
        "    #gradient descendt\n",
        "    gdmodel = SGDClassifier().fit(X_IMDB_train_tfidf, Y_train)\n",
        "    y_pred = gdmodel.predict(X_IMDB_test_tfidf)\n",
        "    acc =  evaluate_acc(y_pred, Y_test)\n",
        "    print('GD ', perc, '=', acc)\n",
        "    GDlist.append(acc)\n",
        "\n",
        "    #SVC\n",
        "    svcmodel = svm.LinearSVC().fit(X_IMDB_train_tfidf, Y_train)\n",
        "    y_pred = svcmodel.predict(X_IMDB_test_tfidf)\n",
        "    acc =  evaluate_acc(y_pred, Y_test)\n",
        "    print('SVC ', perc, '=', acc)\n",
        "    SVClist.append(acc)\n",
        "\n",
        "  plt.plot(train_perc, NBlist, 'r-', label='Multinomial Naive Bayes',alpha=0.7)\n",
        "  plt.plot(train_perc, Bernlist, 'p-', label='Bernoulli',alpha=0.7)\n",
        "  plt.plot(train_perc, LRoptlist, 'k-', label='Optimized Logistic Regression',alpha=0.7)\n",
        "  plt.plot(train_perc, LRlist, 'b-', label='Default Logistic Regression',alpha=0.7)\n",
        "  plt.plot(train_perc, GDlist, 'y-', label='Gradient Descent Classifier',alpha=0.7)\n",
        "  plt.plot(train_perc, SVClist, 'g-', label='SVC',alpha=0.7)\n",
        "  plt.xticks(np.arange(min(train_perc), max(train_perc)+1, 5.0))\n",
        "  plt.xlabel('Percent Training Data')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}