{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP551 Project 3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MCHl00W8TMnG",
        "9RbW7guFGQd0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afratzscher/COMP551/blob/main/COMP551_Project_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mex8yW53Znvm"
      },
      "source": [
        "#COMP551 Project 3: Classification of Image Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBKrYhzFPxKD"
      },
      "source": [
        "#HOW TO RUN\n",
        "To run the code, first run the \"import packages\" code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCHl00W8TMnG"
      },
      "source": [
        "# IMPORT PACKAGES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwT_LFxHTQzv"
      },
      "source": [
        "Import packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiOlUdf8r0Ow"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils # for one-hot coding\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.core.debugger import set_trace         #for debugging \n",
        "\n",
        "np.random.seed(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RbW7guFGQd0"
      },
      "source": [
        "# Task 1: Acquire data\n",
        "NOTE: you must run the methods in order (import, then vectorize, then normalize), as vectorization will vectorize a second time if run twice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOQwIsH_Ipz9"
      },
      "source": [
        "Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Vnuykl9GQKQ",
        "outputId": "2543da68-bd1f-4048-d7e3-56098dec3f00"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    # Returns tuple of Numpy arrays: (x_train, y_train), (x_test, y_test).\n",
        "    # x_train, x_test: uint8 arrays of grayscale image data with shapes (num_samples, 28, 28).\n",
        "      # instance of x = 28x28 array with values between 0 (black) and 255 (white) -> is an image of a handwritten number\n",
        "    # y_train, y_test: uint8 arrays of digit labels (integers in range 0-9) with shapes (num_samples,).\n",
        "print(\"Number of training instances: \" + str(len(x_train)))\n",
        "print(\"Number of testing instances: \" + str(len(x_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Number of training instances: 60000\n",
            "Number of testing instances: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nblqKAVmIrM5"
      },
      "source": [
        "Plotting a few examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "bzc1F2IMIsEk",
        "outputId": "4f31b6db-bd4d-4b42-a7bd-7faeb8a9c886"
      },
      "source": [
        "fig = plt.figure()\n",
        "for i in range(9):\n",
        "  plt.subplot(3,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(x_train[i], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Digit: {}\".format(y_train[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__18V0YKJxGq"
      },
      "source": [
        "Vectorize x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nSD66t_JxyA",
        "outputId": "382c3fc1-9ece-4830-94be-0abae41122f6"
      },
      "source": [
        "print(\"Initial x_train shape: \", x_train.shape) # initial shape = (60000, 28, 28)\n",
        "# print(\"y_train shape\", y_train.shape)\n",
        "print(\"Initial x_test shape\", x_test.shape) # initial shape = (10000, 28, 28)\n",
        "# print(\"y_test shape\", y_test.shape)\n",
        "\n",
        "#want to change from 60000x28x28 to 60000x784 (784 = 28x28) to get vector\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "print(\"Vectorized x_train shape: \", x_train.shape)\n",
        "print(\"Vectorized x_test shape: \", x_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial x_train shape:  (60000, 28, 28)\n",
            "Initial x_test shape (10000, 28, 28)\n",
            "Vectorized x_train shape:  (60000, 784)\n",
            "Vectorized x_test shape:  (10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0F8t6sqkWYFD"
      },
      "source": [
        "Use one-hot coding for y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1a1ONOnWdKL",
        "outputId": "1ef50a1f-f831-4ac7-9f9e-a7d8840fef9c"
      },
      "source": [
        "# Adapted from https://nextjournal.com/gkoehler/digit-recognition-with-keras\n",
        "num_classes = 10 # 10 b/c digits 0-9\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
        "y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "print(\"Shape after one-hot encoding: \", y_train.shape)\n",
        "\n",
        "# means instead of instance 1 have label '5', has label [0,0,0,0,0,1,0,0,0,0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape before one-hot encoding:  (60000,)\n",
            "Shape after one-hot encoding:  (60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKBViX5eRgAc"
      },
      "source": [
        "Normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R57WoCg6RgzP",
        "outputId": "f433cedb-d039-4722-f500-5d6a724a3f53"
      },
      "source": [
        "# option 1: normalize by changing range from 0-255 to 0-1 (from https://nextjournal.com/gkoehler/digit-recognition-with-keras)\n",
        "# x_train /= 255\n",
        "# x_test /= 255\n",
        "\n",
        "# option 2: normalize using x-mean/stdev https://cs231n.github.io/neural-networks-2/#datapre\n",
        "print(\"X_train before normalization\")\n",
        "print(\"mean: \", np.mean(x_train))\n",
        "print(\"stdev: \", np.std(x_train))\n",
        "# _ = plt.hist(x_train)\n",
        "# plt.show()\n",
        "x_train -= np.mean(x_train, axis=0)\n",
        "x_train /= np.std(x_train)\n",
        "\n",
        "print(\"X_train after normalization\")\n",
        "print(\"mean: \", np.mean(x_train))\n",
        "print(\"stdev: \", np.std(x_train))\n",
        "# _ = plt.hist(x_train)\n",
        "# plt.show()\n",
        "\n",
        "print(\"X_test before normalization\")\n",
        "print(\"mean: \", np.mean(x_test))\n",
        "print(\"stdev: \", np.std(x_test))\n",
        "# _ = plt.hist(x_test)\n",
        "# plt.show()\n",
        "x_test -= np.mean(x_test, axis=0)\n",
        "x_test /= np.std(x_test)\n",
        "\n",
        "print(\"X_test after normalization\")\n",
        "print(\"mean: \", np.mean(x_test))\n",
        "print(\"stdev: \", np.std(x_test))\n",
        "# _ = plt.hist(x_test)\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train before normalization\n",
            "mean:  33.318447\n",
            "stdev:  78.567444\n",
            "X_train after normalization\n",
            "mean:  -7.1363386e-09\n",
            "stdev:  1.0\n",
            "X_test before normalization\n",
            "mean:  33.79124\n",
            "stdev:  79.172455\n",
            "X_test after normalization\n",
            "mean:  -3.9071452e-10\n",
            "stdev:  1.0000006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwisz_rkW5eq"
      },
      "source": [
        "#Task 2: Implement MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8IQV3-bZr7U"
      },
      "source": [
        "###EXAMPLE: MLP for multi-class softmax classification\n",
        "shows that data format correct\n",
        "\n",
        "based on: https://medium.com/data-science-bootcamp/multilayer-perceptron-mlp-vs-convolutional-neural-network-in-deep-learning-c890f487a8f1\n",
        "\n",
        "gets 99% accuracy, 20 epochs (iterations)\n",
        "\n",
        "also tried using 0 for initial weights -> leads to low accuracy -> better results with glorotNormal and glorotUniform (see https://keras.io/api/layers/initializers/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ5oqa07Y-Lf",
        "outputId": "4a7ba09c-8e37-47e4-8a6b-72a3fb75ef45"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "from keras.initializers import GlorotNormal\n",
        "\n",
        "num_classes = 10\n",
        "model = Sequential() #has implicit input layer added\n",
        "initializer = GlorotNormal()\n",
        "model.add(Dense(64, activation='relu', kernel_initializer=initializer)) #hidden layer 1\n",
        "# model.add(Dropout(0.5))\n",
        "  # b/c randomly dropout each unit with probability 0.5\n",
        "  # runs faster BUT less accurate (95 vs 99% accuracy)\n",
        "model.add(Dense(64,activation='relu', kernel_initializer=initializer)) #hidden layer 2\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax', kernel_initializer=initializer))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
        "score = model.evaluate(x_test, y_test, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 4s 3ms/step - loss: 0.4367 - accuracy: 0.8660\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.1082 - accuracy: 0.9669\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0717 - accuracy: 0.9776\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0537 - accuracy: 0.9833\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 3s 3ms/step - loss: 0.0388 - accuracy: 0.9887\n",
            "157/157 [==============================] - 1s 2ms/step - loss: 0.0920 - accuracy: 0.9743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erezGQxWADKX"
      },
      "source": [
        "###MLP Class\n",
        "based on https://github.com/rcassani/mlp-example/blob/master/mlp.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsvgLdGqc9Wg",
        "outputId": "236c78d1-e395-4b0d-e75f-e1cc59547c4c"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "nn = MLPClassifier(hidden_layer_sizes=(64,64), activation='relu',\n",
        "                   solver='sgd', learning_rate='constant',  random_state=43,\n",
        "                   max_iter=20)\n",
        "# print \"Training model...\"\n",
        "nn.fit(x_train,y_train)\n",
        "acc = nn.score(x_test, y_test)\n",
        "#pred = nn.predict(Xval)\n",
        "print(\"Validation accuracy: {:.2f}%\".format(acc * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy: 93.81%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XioX6pPMc8ko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6877dd-6bd9-4f44-8b7b-7b9772e425ea"
      },
      "source": [
        "#TO DO: gradient descent\n",
        "#TO DO: decide on start bias (currently 0.01 b/c have issues otherwise for Relu input if 0)\n",
        "\n",
        "# DONE: weight intialization using Glorot uniform or He normal\n",
        "# DONE: forward propagation (to check)\n",
        "# DONE: backpropagation (for weights and biases)\n",
        "# DONE: predict\n",
        "\n",
        " #IDEA: for each iteration, get gradient using backpropagation\n",
        "      # then, use minibatch GD to update weights\n",
        "        #HYPERPARAMS TO TUNE: learning rate, number iterations (aka epochs), adaptive/constant learning rate\n",
        "\n",
        "# IMPORTANT: assumes that all layers have same activation function (except last = softmax)\n",
        "\n",
        "def evaluate_accuracy(pred, label):\n",
        "  #convert one-hot encoding to ints\n",
        "  pred = np.argmax(pred, axis=1)\n",
        "  label = np.argmax(label, axis=1)\n",
        "  accuracy = np.sum(pred == label)/len(pred)\n",
        "  return accuracy\n",
        "\n",
        "sigmoid = lambda z: 1./ (1 + np.exp(-z))\n",
        "tanh = lambda z: np.tanh(z)\n",
        "def softmax(z):\n",
        "    ez = np.exp(z)\n",
        "    ez_sum = np.sum(np.exp(z))\n",
        "    return ez / ez_sum\n",
        "def relu(z):\n",
        "  return np.maximum(0, z)\n",
        "\n",
        "def softmax_deriv(z):\n",
        "  return (softmax(z) * (1 - softmax(z)))\n",
        "  # softmax derivative only in case that NOT for last layer \n",
        "  # (b/c for last layer, dL/dy * dy/du = (yh - y), so use directly)\n",
        "def sigmoid_deriv(z):\n",
        "  return (sigmoid(z) * (1 - sigmoid(z)))\n",
        "def relu_deriv(z):\n",
        "  return 1 * (z > 0)\n",
        "def tanh_deriv(z):\n",
        "  return 1 - (tanh(z))**2\n",
        "\n",
        "class MLP:\n",
        "  def initialize_weights(self): \n",
        "    # https://stats.stackexchange.com/questions/373136/softmax-weights-initialization\n",
        "      # says use Glorot uniform (aka Xavier uniform) for sigmoid/tanh/none/softmax, He normal for ReLu\n",
        "    # adapted from https://visualstudiomagazine.1105cms01.com/articles/2019/09/05/neural-network-glorot.aspx\n",
        "    # https://github.com/rcassani/mlp-example/blob/51953043302b4cb5467bee510780a867830b2bdb/mlp.py#L245\n",
        "    self.weights = []\n",
        "    size_next_layer = self.size_layers.copy()\n",
        "    size_next_layer.pop(0) # removes input layer size\n",
        "    i = 0 #index of activation\n",
        "    for layer, next in (zip(self.size_layers, size_next_layer)): \n",
        "      if self.activation[i] == 'relu':\n",
        "        # use He normal if relu\n",
        "        epsilon = np.sqrt(2.0 / (layer * next))\n",
        "        # temp = epsilon * (np.random.rand(next, layer + 1))\n",
        "        temp = epsilon * (np.random.rand(next, layer)) # no bias\n",
        "      else:\n",
        "        #else, use Glorot uniform \n",
        "        epsilon = 4.0 * np.sqrt(6) / np.sqrt(layer + next)\n",
        "        # temp = epsilon * ((np.random.rand(next, layer + 1) * 2.0) - 1) # +1 b/c use bias\n",
        "        temp = epsilon * ((np.random.rand(next, layer) * 2.0) - 1) # no bias\n",
        "      self.weights.append(temp)\n",
        "      i+=1\n",
        "    return self.weights\n",
        "  \n",
        "  def initialize_biases(self):\n",
        "    # lecture recommends initializing to small value > 0 so that Relu input > 0\n",
        "    self.biases = [0.01] * len(self.weights)\n",
        "    return self.biases\n",
        "\n",
        "  def __init__(self, num_hidden=0, size_layers=[], activation_function='none', learning_rate=0.01):\n",
        "    self.reg = None # no regularization\n",
        "    self.num_layers = 2 + len(size_layers) # only have units for hidden layers (so add input + output layers)\n",
        "    self.size_layers = size_layers\n",
        "    self.for_init_size_layes = self.size_layers.copy()\n",
        "    self.num_hidden = num_hidden\n",
        "    self.activation = [activation_function for x in range(num_hidden)]\n",
        "    self.activation.append('softmax')\n",
        "    self.learning_rate = learning_rate\n",
        "\n",
        "  def add_input_output_size(self, x_train, y_train):\n",
        "    temp = self.size_layers\n",
        "    temp.append(num_classes)\n",
        "    temp.insert(0, x_train.shape[1])\n",
        "    self.size_layers = temp  \n",
        "    self.num_input = x_train.shape[1]\n",
        "\n",
        "  def apply_act(self, act_fn, input, weight, bias):\n",
        "    bias_idx = input.shape[1]\n",
        "    if act_fn == 'softmax':\n",
        "      return softmax(np.dot(input, weight.T) + bias)\n",
        "    elif act_fn == 'relu':\n",
        "      return relu(np.dot(input, weight.T) + bias)\n",
        "    elif act_fn == 'tanh':\n",
        "      return tanh(np.dot(input, weight.T) + bias)\n",
        "    elif act_fn == 'sigmoid':\n",
        "      return sigmoid(np.dot(input, weight.T) + bias)\n",
        "      # sigmoid of (input *. weight (no bias) + bias)\n",
        "    return (np.dot(input, weight.T + bias))\n",
        "      # return wx if no activation function\n",
        "\n",
        "  def forward_propagate(self, x):\n",
        "    vals = []\n",
        "    input = x\n",
        "    vals.append(input)\n",
        "      ## FORWARD PROPAGATE FIRST ##\n",
        "    for idx in range(len(self.activation)):\n",
        "      val = self.apply_act(self.activation[idx], input, self.weights[idx], self.biases[idx])\n",
        "      input = val\n",
        "      vals.append(val)\n",
        "    return vals\n",
        "\n",
        "  def get_gradient(self, x, y):\n",
        "    # NOTE: if no hidden layers, have only softmax\n",
        "    ## FORWARD PROPAGATE FIRST ##\n",
        "    vals = self.forward_propagate(x)\n",
        "    yh = vals[-1]\n",
        "    vals.pop() # removes yh\n",
        "\n",
        "    ## THEN BACK PROPAGATE ##\n",
        "    weight_gradient = []\n",
        "    bias_gradient = []\n",
        "      # see for bias gradient explanation: https://datascience.stackexchange.com/questions/20139/gradients-for-bias-terms-in-backpropagation\n",
        "    N,D = x.shape\n",
        "    for i in range(0, len(self.activation)):\n",
        "      if i == 0:\n",
        "        dy = (yh-y) # b/c dL/dy dy/du = (yh-y) \n",
        "        prev = dy\n",
        "        dw = np.dot(vals[-1].T, prev)/N\n",
        "        weight_gradient.append(dw)\n",
        "        bias_gradient.append(np.sum(prev, axis=0, keepdims=True))\n",
        "      else:\n",
        "        idx = len(self.activation)-1 - i\n",
        "        if self.activation[idx] == 'softmax':\n",
        "          d_layer = softmax_deriv(vals[idx+1])\n",
        "        elif self.activation[idx] == 'relu':\n",
        "          d_layer = relu_deriv(vals[idx+1])\n",
        "        elif self.activation[idx] == 'tanh':\n",
        "          d_layer = tanh_deriv(vals[idx+1])\n",
        "        elif self.activation[idx] == 'sigmoid':\n",
        "          d_layer = sigmoid_deriv(vals[idx+1])\n",
        "        tmp = np.dot(prev, self.weights[idx+1]) # assumes weight doesnt contain bias (separate)\n",
        "        prev = tmp * d_layer\n",
        "        weight_gradient.append(np.dot(vals[idx].T, prev)/N)\n",
        "        bias_gradient.append(np.sum(prev, axis=0, keepdims=True))\n",
        "        # bias gradient = error (whereas weight gradient = vals * error)\n",
        "    return weight_gradient, bias_gradient\n",
        "    #NOTE: Gradients in reverse order (dw, dv, ...)\n",
        "    #NOTE: weights = [v, w, ...]\n",
        "\n",
        "  def fit(self, x_train, y_train, epochs=3, batch_size=128):\n",
        "    for iteration in range(epochs):\n",
        "      print('iter',iteration)\n",
        "      if iteration == 0: # adds size of input/output to size_layers variable on first iteration\n",
        "        self.add_input_output_size(x_train, y_train)\n",
        "        self.initialize_weights() # only initialize on first iteration\n",
        "        self.initialize_biases()\n",
        "\n",
        "      # get gradient for weight and bais from backpropagation\n",
        "      self.weight_gradient, self.bias_gradient = self.get_gradient(x_train, y_train)\n",
        "      break\n",
        "      #then need to update weights using mini-batch GD: TO DO!\n",
        "    return\n",
        "\n",
        "  def predict(self, x_test):\n",
        "    vals = self.forward_propagate(x_test)\n",
        "    yh = vals[-1]\n",
        "    yh = (yh == yh.max(axis=1, keepdims=1)).astype(float)\n",
        "    return yh\n",
        "\n",
        "##RUNNING MLP##\n",
        "# model2 = MLP()\n",
        "model2 = MLP(num_hidden=2, size_layers=[64,64], activation_function='sigmoid', learning_rate=0.01)\n",
        "# model2 = MLP(1, [64], 'sigmoid')\n",
        "print(model2.activation)\n",
        "model2.fit(x_train[:100], y_train[:100])\n",
        "yh = model2.predict(x_test)\n",
        "accuracy = evaluate_accuracy(yh, y_test)\n",
        "print(\"Accuracy: %s\" % '{0:.3%}'.format(accuracy))\n",
        "# print(len(model2.weight_gradient)) # should be equal to # of weights (e.g. 2 hidden layers = 3)\n",
        "# print(len(model2.bias_gradient))\n",
        "\n",
        "\n",
        "# print(model2.size_layers)\n",
        "# print(model2.weights[0].shape) # get list of len 3, with each element having size 64x785 (785 b/c 784 + 1 for bias)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sigmoid', 'sigmoid', 'softmax']\n",
            "iter 0\n",
            "Accuracy: 8.540%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}