{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mex8yW53Znvm"
   },
   "source": [
    "#First Project - COMP 551\n",
    "\n",
    "[Link](https://mycourses2.mcgill.ca/content/enforced/495331-15812.202101/551P1W21.pdf?_&d2lSessionVal=DARoDapct3pTW4gYIBVudaEKF) to Assignment instructions.\n",
    "\n",
    "[Link](https://colab.research.google.com/github/mravanba/comp551-notebooks/blob/master/KNN.ipynb) to example of nearest neighbors implementation.\n",
    "\n",
    "[Link](https://colab.research.google.com/github/mravanba/comp551-notebooks/blob/master/DecisionTree.ipynb) to example of decision tree implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHQJKhrULUbZ"
   },
   "source": [
    "\n",
    "# Abstract\n",
    "The objective of this projeect is to determine which classification method among K-Nearest Neighbors (KNN) and Decision Trees (DT) is most accurate. We made predictions useing two benchmark datasets, one on breast cancer diagnosis and the other concerning hepatitis prognosis. First we discuss the cleaning of the datasets. Then, after implementing and optimizing both methods, we select the most highly predictive features, best cost functions, distance functions and hyperparameters. Ultimately, we determine that KNN was better at predicting malignant cases of breast cancer, as well as predicting lethal cases of hepatitis, than DT. Additionally, we discuss the effects of imbalanced data on these methods.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBKrYhzFPxKD"
   },
   "source": [
    "#HOW TO RUN\n",
    "\n",
    "To run the code, first run the \"import packages\" code. Additionally, run the \"KNN class\" and \"Decision Tree class\", as they contain the class definitions. \n",
    "\n",
    "Note that you must run the \"convert numpy array\" before training the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCHl00W8TMnG"
   },
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwT_LFxHTQzv"
   },
   "source": [
    "Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiOlUdf8r0Ow"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "import hashlib as hl\n",
    "\n",
    "import statistics\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.core.debugger import set_trace         #for debugging \n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v4D6DXvTW7W"
   },
   "source": [
    "Setup Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4w3vRmqyh69j"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Wqkv-w0_Eju"
   },
   "source": [
    "**This section is to upload the data from drive**\n",
    "\n",
    "\n",
    "If you want to see how to upload on drive  look [here](https://colab.research.google.com/drive/18QoifMK96VraKlQgOz3_7iAXAeURoR9u#scrollTo=2Wqkv-w0_Eju&line=2&uniqifier=1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShGr84onUpYH"
   },
   "source": [
    "#KNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv__2q9aMR2x"
   },
   "outputs": [],
   "source": [
    "#define the metric we will use to measure similarity\n",
    "#if the input shapes are [1,N1,F] and [N2,1,F] then output shape is [N2,N1]\n",
    "#as numpy supports broadcasting with arithmetic operations\n",
    "#for more on numpy broadcasting refer to: https://numpy.org/doc/stable/user/basics.broadcasting.html  \n",
    "\n",
    "euclidian = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
    "hamming = lambda x1, x2: 0 if x1 != x2 else 1\n",
    "# hamming_distance\n",
    "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, K=1, dist_fn= euclidian):\n",
    "        self.dist_fn = dist_fn\n",
    "        self.K = K\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        ''' Store the training data using this method as it is a lazy learner'''\n",
    "        self.x = x\n",
    "        self.y = y.astype(int) # class should always be int (0 or 1)\n",
    "        self.C = int(np.max(y) + 1)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        ''' Makes a prediction using the stored training data and the test data given as argument'''\n",
    "        num_test = x_test.shape[0]\n",
    "        #calculate distance between the training & test samples and returns an array of shape [num_test, num_train]\n",
    "        \n",
    "        distances = self.dist_fn(self.x[None,:,:], x_test[:,None,:])\n",
    "        #ith-row of knns stores the indices of k closest training samples to the ith-test sample \n",
    "        knns = np.zeros((num_test, self.K), dtype=int)\n",
    "        #ith-row of y_prob has the probability distribution over C classes\n",
    "        y_prob = np.zeros((num_test, int(self.C)))\n",
    "\n",
    "        for i in range(num_test):\n",
    "            knns[i,:] = np.argsort(distances[i])[:self.K]\n",
    "            y_prob[i,:] = np.bincount(self.y[knns[i,:]], minlength=self.C) #counts the number of instances of each class in the K-closest training samples\n",
    "        y_prob /= np.sum(y_prob, axis=-1, keepdims=True)\n",
    "        \n",
    "        #simply divide by K to get a probability distribution\n",
    "        y_prob /= self.K\n",
    "        return y_prob, knns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhvAzOx8LXht"
   },
   "source": [
    "# Decision Tree Class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwnO98-7vUW-"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, data_indices, parent):\n",
    "        self.data_indices = data_indices                    #stores the data indices which are in the region defined by this node\n",
    "        self.left = None                                    #stores the left child of the node \n",
    "        self.right = None                                   #stores the right child of the node\n",
    "        self.split_feature = None                           #the feature for split at this node\n",
    "        self.split_value = None                             #the value of the feature for split at this node\n",
    "        if parent:\n",
    "            self.depth = parent.depth + 1                   #obtain the dept of the node by adding one to dept of the parent \n",
    "            self.num_classes = parent.num_classes           #copies the num classes from the parent \n",
    "            self.data = parent.data                         #copies the data from the parent\n",
    "            self.labels = parent.labels.astype(int)         #copies the labels from the parent\n",
    "            class_prob = np.bincount(self.labels[data_indices], minlength=self.num_classes) #this is counting frequency of different labels in the region defined by this node\n",
    "            self.class_prob = class_prob / np.sum(class_prob)  #stores the class probability for the node\n",
    "            #note that we'll use the class probabilites of the leaf nodes for making predictions after the tree is built\n",
    "\n",
    "\n",
    "def greedy_test(node, cost_fn):\n",
    "    #initialize the best parameter values\n",
    "    best_cost = np.inf\n",
    "    best_feature, best_value = None, None\n",
    "    num_instances, num_features = node.data.shape\n",
    "    #sort the features to get the test value candidates by taking the average of consecutive sorted feature values \n",
    "    data_sorted = np.sort(node.data[node.data_indices],axis=0)\n",
    "    test_candidates = (data_sorted[1:] + data_sorted[:-1]) / 2.\n",
    "    for f in range(num_features):\n",
    "        #stores the data corresponding to the f-th feature\n",
    "        data_f = node.data[node.data_indices, f]\n",
    "        for test in test_candidates[:,f]:\n",
    "            #Split the indices using the test value of f-th feature\n",
    "            left_indices = node.data_indices[data_f <= test]\n",
    "            right_indices = node.data_indices[data_f > test]\n",
    "            #we can't have a split where a child has zero element\n",
    "            #if this is true over all the test features and their test values  then the function returns the best cost as infinity\n",
    "            if len(left_indices) == 0 or len(right_indices) == 0:                \n",
    "                continue\n",
    "            #compute the left and right cost based on the current split                                                         \n",
    "            left_cost = cost_fn(node.labels[left_indices])\n",
    "            right_cost = cost_fn(node.labels[right_indices])\n",
    "            num_left, num_right = left_indices.shape[0], right_indices.shape[0]\n",
    "            #get the combined cost using the weighted sum of left and right cost\n",
    "            cost = (num_left * left_cost + num_right * right_cost)/num_instances\n",
    "            #update only when a lower cost is encountered\n",
    "            if cost < best_cost:\n",
    "                best_cost = cost\n",
    "                best_feature = f\n",
    "                best_value = test\n",
    "    return best_cost, best_feature, best_value\n",
    "\n",
    "\n",
    "\n",
    "#computes misclassification cost by subtracting the maximum probability of any class\n",
    "def cost_misclassification(labels):\n",
    "    labels = labels.astype(int)\n",
    "    counts = np.bincount(labels) \n",
    "    class_probs = counts / np.sum(counts)\n",
    "    #you could compress both the steps above by doing class_probs = np.bincount(labels) / len(labels)\n",
    "    return 1 - np.max(class_probs)\n",
    "\n",
    "#computes entropy of the labels by computing the class probabilities\n",
    "def cost_entropy(labels):\n",
    "    labels = labels.astype(int)\n",
    "    class_probs = np.bincount(labels) / len(labels)\n",
    "    class_probs = class_probs[class_probs > 0]              #this steps is remove 0 probabilities for removing numerical issues while computing log\n",
    "    return -np.sum(class_probs * np.log(class_probs))       #expression for entropy -\\sigma p(x)log[p(x)]\n",
    "\n",
    "#computes the gini index cost\n",
    "def cost_gini_index(labels):\n",
    "    labels = labels.astype(int)\n",
    "    class_probs = np.bincount(labels) / len(labels)\n",
    "    return 1 - np.sum(np.square(class_probs))               #expression for gini index 1-\\sigma p(x)^2\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, num_classes=None, max_depth=3, cost_fn=cost_entropy, min_leaf_instances=1, min_cost = 0.001):\n",
    "        self.max_depth = max_depth      #maximum dept for termination \n",
    "        self.root = None                #stores the root of the decision tree \n",
    "        self.cost_fn = cost_fn          #stores the cost function of the decision tree \n",
    "        self.num_classes = num_classes  #stores the total number of classes\n",
    "        self.min_leaf_instances = min_leaf_instances  #minimum number of instances in a leaf for termination\n",
    "        self.min_cost = min_cost #############################\n",
    "        \n",
    "    def fit(self, data, labels):\n",
    "        pass                            #pass in python 3 means nothing happens and the method here is empty\n",
    "    \n",
    "    def predict(self, data_test):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "def fit(self, data, labels):\n",
    "    self.data = data\n",
    "    self.labels = labels.astype(int)\n",
    "    if self.num_classes is None:\n",
    "        self.num_classes = int(np.max(labels) + 1)\n",
    "    #below are initialization of the root of the decision tree\n",
    "    self.root = Node(np.arange(data.shape[0]), None)\n",
    "    self.root.data = data\n",
    "    self.root.labels = labels\n",
    "    self.root.num_classes = self.num_classes\n",
    "    self.root.depth = 0\n",
    "    #to recursively build the rest of the tree\n",
    "    self._fit_tree(self.root)\n",
    "    return self\n",
    "\n",
    "def _fit_tree(self, node):\n",
    "    #This gives the condition for termination of the recursion resulting in a leaf node\n",
    "    if node.depth == self.max_depth or len(node.data_indices) <= self.min_leaf_instances :\n",
    "        return\n",
    "    #greedily select the best test by minimizing the cost\n",
    "    cost, split_feature, split_value = greedy_test(node, self.cost_fn)\n",
    "    #if the cost returned is infinity it means that it is not possible to split the node and hence terminate\n",
    "    if np.isinf(cost):\n",
    "        return\n",
    "\n",
    "    if cost <= self.min_cost: ####################################\n",
    "      return\n",
    "    #print(f'best feature: {split_feature}, value {split_value}, cost {cost}') ##########################################\n",
    "    #to get a boolean array suggesting which data indices corresponding to this node are in the left of the split\n",
    "    test = node.data[node.data_indices,split_feature] <= split_value\n",
    "    #store the split feature and value of the node\n",
    "    node.split_feature = split_feature\n",
    "    node.split_value = split_value\n",
    "    #define new nodes which are going to be the left and right child of the present node\n",
    "    left = Node(node.data_indices[test], node)\n",
    "    right = Node(node.data_indices[np.logical_not(test)], node)\n",
    "    #recursive call to the _fit_tree()\n",
    "    self._fit_tree(left)\n",
    "    self._fit_tree(right)\n",
    "    #assign the left and right child to present child\n",
    "    node.left = left\n",
    "    node.right = right\n",
    "\n",
    "DecisionTree.fit = fit\n",
    "DecisionTree._fit_tree = _fit_tree\n",
    "\n",
    "\n",
    "\n",
    "def predict(self, data_test):\n",
    "    class_probs = np.zeros((data_test.shape[0], self.num_classes))\n",
    "    for n, x in enumerate(data_test):\n",
    "        node = self.root\n",
    "        #loop along the dept of the tree looking region where the present data sample fall in based on the split feature and value\n",
    "        while node.left:\n",
    "            if x[node.split_feature] <= node.split_value:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        #the loop terminates when you reach a leaf of the tree and the class probability of that node is taken for prediction\n",
    "        class_probs[n,:] = node.class_prob\n",
    "    return class_probs\n",
    "\n",
    "DecisionTree.predict = predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDHKjMknStUp"
   },
   "source": [
    "# DATASET 1: Breast Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euIpsUerlUJ4"
   },
   "source": [
    "##Task 1: Acquire, preprocess, and analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnkX4OHWS2MN"
   },
   "source": [
    "###Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDfaQqhcTiYn"
   },
   "source": [
    "Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EhC5qYTBEoq"
   },
   "outputs": [],
   "source": [
    "\n",
    "#unhash the following snippet if you want to work straight from the csv file on google drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "\n",
    "df1 = pd.read_csv(\"/content/gdrive/My Drive/Colab Notebooks/Assignment1/breast_cancer_wisconsin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpU3hV50sY4J"
   },
   "source": [
    "\n",
    "Removing rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdLGU6qisgjD"
   },
   "outputs": [],
   "source": [
    "#df1 = pd.read_csv(\"Assignment1/breast_cancer_wisconsin.csv\")  # df1 represents the dataframe for the breast_cancer_wisconsin csv file\n",
    "df1.head(5)                                       # Observe first 5 rows\n",
    "df1.isnull().any()                                # Check if there is any missing values in each column \n",
    "df1.dtypes\n",
    "\n",
    "print(df1.shape[0],\" rows in df1\\n\")\n",
    "\n",
    "df1 = df1.replace('?', np.nan)\n",
    "df1 = df1.dropna() \n",
    "\n",
    "print(df1.shape[0],\" rows left in df1\\n\")         # 16 rows have been removed\n",
    "\n",
    "df1.reset_index(drop=True, inplace=True) #reset indices (so first instance at index 0, next at index 1...)\n",
    "\n",
    "df1['Class'] = df1['Class'].replace([2], 0) #benign\n",
    "df1['Class'] = df1['Class'].replace([4], 1) #malignant\n",
    "  # replace 2 with 0, 4 with 1 (for decision boundary later)\n",
    "\n",
    "#delete id column\n",
    "del df1['id']\n",
    "\n",
    "df1 = df1.astype(float) # all values are float (instead of string)\n",
    "df1 = df1.astype({\"Class\": int})\n",
    "\n",
    "#print(\"\\n\",df1_np)\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSZbfgDeHvtj"
   },
   "source": [
    "Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1Fb0srFH3ED"
   },
   "outputs": [],
   "source": [
    "# removes outliers\n",
    "df1 = df1[(np.abs(stats.zscore(df1))<3).all(axis=1)]\n",
    "print(df1.shape[0], ' rows after outlier removal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxRN-5mbz7U1"
   },
   "source": [
    "###Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7osmg68xz-oC"
   },
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBbj6Kn90M__"
   },
   "source": [
    "Number of Occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mm4xVQ-D0Kp4"
   },
   "outputs": [],
   "source": [
    "# 0 = benign, 1 = malignant\n",
    "benign = df1.loc[(df1['Class'] == 0)] \n",
    "num_ben = benign.shape[0]\n",
    "malignant = df1.loc[(df1['Class'] == 1)] \n",
    "num_malig = malignant.shape[0]\n",
    "\n",
    "print('Total # Patients: ', num_ben+num_malig)\n",
    "print('# Benign: ', num_ben)\n",
    "print('# Malignant: ', num_malig)\n",
    "per_malig = (num_malig)/(num_ben+num_malig) * 100\n",
    "print(\"% Malignant: \" + str(int(per_malig)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk0Udd4g59eX"
   },
   "source": [
    "####Distribution by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVaIQ77y59eY"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df1['Clump_Thickness'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ms9sKh6B59eZ"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df1['Uniformity_of_Cell_Size'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yq6ib8As59eZ"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df1['Marginal_Adhesion'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-tdstKn59eZ"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df1['Single_Epithelial_Cell_Size'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTtQAfs459ea"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df1['Bare_Nuclei'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9kUKpFG59ea"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df1['Bland_Chromatin'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaNuPqrv59ea"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df1['Normal_Nucleoli'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDKWUV6u59ea"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df1['Mitoses'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1p_3fur59eb"
   },
   "source": [
    "####Positive vs. Negative Distribution by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dw9pnLoC59eb"
   },
   "outputs": [],
   "source": [
    "pos = df1[df1['Class'] == 1]\n",
    "neg = df1[df1['Class'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcV6W3E-59eb"
   },
   "outputs": [],
   "source": [
    "# by clump thickness\n",
    "sns.histplot(pos['Clump_Thickness'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['Clump_Thickness'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zw8uhyvq59eb"
   },
   "outputs": [],
   "source": [
    "# by cell size\n",
    "sns.histplot(pos['Uniformity_of_Cell_Size'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['Uniformity_of_Cell_Size'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skCO_f7Y59ec"
   },
   "outputs": [],
   "source": [
    "# by cell shape\n",
    "sns.histplot(pos['Uniformity_of_Cell_Shape'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['Uniformity_of_Cell_Shape'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-WQeV8S59ec"
   },
   "outputs": [],
   "source": [
    "# by marginal adhesion \n",
    "sns.histplot(pos['Marginal_Adhesion'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['Marginal_Adhesion'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0khDYtV59ec"
   },
   "outputs": [],
   "source": [
    "# by epithelia cell size  \n",
    "sns.histplot(pos['Single_Epithelial_Cell_Size'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['Single_Epithelial_Cell_Size'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqRaRzNL59ec"
   },
   "outputs": [],
   "source": [
    "# by bare nucleoli\n",
    "sns.histplot(pos['Bare_Nuclei'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['Bare_Nuclei'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vI6-nksG59ed"
   },
   "outputs": [],
   "source": [
    "# by bland chromatin \n",
    "sns.histplot(pos['Bland_Chromatin'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['Bland_Chromatin'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPIALcF-59ed"
   },
   "outputs": [],
   "source": [
    "# by Normal_Nucleoli\n",
    "sns.histplot(pos['Normal_Nucleoli'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['Normal_Nucleoli'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahqTvywz59ed"
   },
   "outputs": [],
   "source": [
    "# by Mitoses\n",
    "sns.histplot(pos['Mitoses'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['Mitoses'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_p5jguInUHR4"
   },
   "source": [
    "###Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "im4qbVpuUJDR"
   },
   "outputs": [],
   "source": [
    "##### Find if there are any correlation between independent variables pairwise\n",
    "corr_matrix = df1.corr()\n",
    "sns.heatmap(corr_matrix,annot = True)\n",
    "plt.show()\n",
    "\n",
    "# \"Class\" (the output) has high correlation with \"Uniformity_of_cell_size\",\n",
    "# \"Uniformity_of_cell_shape\" and \"Bare_Nuclei\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNIx4opeloNF"
   },
   "source": [
    "##Task 2 & 3: Implement the Models and Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BhJadnn1D_Z"
   },
   "source": [
    "###Convert to Numpy Array for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5gwAIxT1CD1"
   },
   "outputs": [],
   "source": [
    "df1_np = df1.to_numpy()                           # Convert df1 from pandas dataframe to numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTjvLLBcLBqU"
   },
   "source": [
    "### KNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWr3VX1udard"
   },
   "source": [
    "####Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6XRHMMMW_bp"
   },
   "outputs": [],
   "source": [
    "#df1 has one column with mixed types (Bare-Nuclei) -> so cant convert all to int\n",
    "x = df1_np[:,:-1] # TA said to train on all features initially for part 2 \n",
    "y = df1_np[:,-1]\n",
    "print(df1)\n",
    "num_instances = df1[[\"Class\"]].size\n",
    "#print(num_instances)\n",
    "(N, D), C = x.shape, np.max(y)+1\n",
    "print(f'instances (N) \\t {N} \\n features (D) \\t {D} \\n classes (C) \\t {C}')\n",
    "\n",
    "inds = np.random.permutation(num_instances)\n",
    "\n",
    "#train-test split\n",
    "x_train, y_train = x[inds[:342]], y[inds[:342]]\n",
    "x_test, y_test = x[inds[342:]], y[inds[342:]]\n",
    "\n",
    "model = KNN(K=20)\n",
    "\n",
    "y_prob, knns = model.fit(x_train, y_train).predict(x_test)\n",
    "\n",
    "#To get hard predictions by choosing the class with the maximum probability\n",
    "y_pred = np.argmax(y_prob,axis=-1)\n",
    "\n",
    "accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
    "\n",
    "print(f'accuracy is {accuracy*100:.1f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyzDET-SdWPY"
   },
   "source": [
    "####Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3gwuv2cdY5O"
   },
   "outputs": [],
   "source": [
    "#visualization (plot for 2 features -> most discriminant features = shape and size (indices 1 and 2))\n",
    "correct = y_test == y_pred\n",
    "incorrect = np.logical_not(correct)\n",
    "plt.scatter(x_train[:,1], x_train[:,2], c=y_train, marker='o', alpha=.2, label='train')\n",
    "plt.scatter(x_test[correct,1], x_test[correct,2], marker='.', c=y_pred[correct], label='correct')\n",
    "plt.scatter(x_test[incorrect,1], x_test[incorrect,2], marker='x', c=y_test[incorrect], label='misclassified')\n",
    "plt.legend()\n",
    "plt.ylabel('Cell size')\n",
    "plt.xlabel('Cell shape')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e44GbzguWnkQ"
   },
   "source": [
    "####Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JW7hgP9oWpD8"
   },
   "outputs": [],
   "source": [
    "#decision boundary for KNN\n",
    "#uses shape and size as data (because can only visualize 2 dimensions)\n",
    "\n",
    "x = df1_np[:, 1:3] # TA said to use 2 features for decision boundary plot\n",
    "y = df1_np[:,-1]\n",
    "(num_instances, num_features), num_classes = x.shape, len(np.unique(y, axis=0))\n",
    "inds = np.random.permutation(num_instances)\n",
    "\n",
    "#train-test split\n",
    "x_train, y_train = x[inds[:342]], y[inds[:342]]\n",
    "x_test, y_test = x[inds[342:]], y[inds[342:]]\n",
    "\n",
    "x0v = np.linspace(np.min(x[:,0]), np.max(x[:,0]),200)\n",
    "x1v = np.linspace(np.min(x[:,1]), np.max(x[:,1]),200)\n",
    "x0, x1 = np.meshgrid(x0v,x1v)\n",
    "x_all = np.vstack((x0.ravel(),x1.ravel())).T\n",
    "\n",
    "model2 = KNN(K=20)\n",
    "y_train_prob = np.zeros((y_train.shape[0], 3))\n",
    "  # need to set to 3 because need 3 channels for RGBA\n",
    "y_train_prob[np.arange(y_train.shape[0]), y_train.astype(int)] = 1\n",
    "y_prob_all, _ = model2.fit(x_train, y_train).predict(x_all)\n",
    "b = np.zeros((y_prob_all.shape[0], 1))\n",
    "y_prob_all = np.concatenate((y_prob_all, b), axis=1)\n",
    "  # adds 3rd dimension for RGBA (filled with 0s) -> otherwise get error\n",
    "\n",
    "y_pred_all = np.zeros_like(y_prob_all)\n",
    "y_pred_all[np.arange(x_all.shape[0]), np.argmax(y_prob_all, axis=-1)] = 1\n",
    "\n",
    "plt.scatter(x_train[:,0], x_train[:,1], c=y_train_prob, marker='o', alpha=1)\n",
    "plt.scatter(x_all[:,0], x_all[:,1], c=y_pred_all, marker='.', alpha=0.01)\n",
    "plt.ylabel('cell size')\n",
    "plt.xlabel('cell shape')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZ8JXxJx6R_S"
   },
   "source": [
    "####Cross-Validation for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V63ViJJc6pJt"
   },
   "outputs": [],
   "source": [
    "def error_rate(pred, label):\n",
    "  accuracy = np.sum(pred == label)/label.shape[0]\n",
    "  return 1-accuracy\n",
    "\n",
    "def cross_validate(n, n_folds=10):\n",
    "    #get the number of data samples in each split\n",
    "    n_val = n // n_folds\n",
    "    inds = np.random.permutation(n)\n",
    "    inds = []\n",
    "    for f in range(n_folds):\n",
    "        tr_inds = []\n",
    "        #get the validation indexes\n",
    "        val_inds = list(range(f * n_val, (f+1)*n_val))\n",
    "        #get the train indexes\n",
    "        if f > 0:\n",
    "            tr_inds = list(range(f*n_val))\n",
    "        if f < n_folds - 1:\n",
    "            tr_inds = tr_inds + list(range((f+1)*n_val, n))\n",
    "        #The yield statement suspends function’s execution and sends a value back to the caller\n",
    "        #but retains enough state information to enable function to resume where it is left off\n",
    "        yield tr_inds, val_inds\n",
    "\n",
    "\n",
    "num_folds = 10\n",
    "(num_instances, num_features), num_classes = x.shape, np.max(y)+1\n",
    "\n",
    "n_test = num_instances// 20\n",
    "n_valid = num_instances// 20\n",
    "  # means train:valid:test = 18:1:1 (train using 90%)\n",
    "inds = np.random.permutation(num_instances)\n",
    "x_test, y_test = x[inds[:n_test]], y[inds[:n_test]]\n",
    "x_rest, y_rest = x[inds[n_test:]], y[inds[n_test:]]\n",
    "n_rest = num_instances - n_test\n",
    "\n",
    "print(x_rest.shape, y_rest.shape)\n",
    "#Plot the mean square error for different K values stored in K_list\n",
    "k_list = range(1,30)\n",
    "err_test, err_valid = np.zeros(len(k_list)), np.zeros((len(k_list), num_folds))\n",
    "for i, k in enumerate(k_list):\n",
    "    #Find the validation errors for num_folds splits for a given K\n",
    "    for f, (tr, val) in enumerate(cross_validate(n_rest, num_folds)):\n",
    "        NN = KNN(K=k)\n",
    "        NN = NN.fit(x_rest[tr], y_rest[tr])\n",
    "        y_prob, knns = NN.predict(x_rest[val])\n",
    "        y_pred = np.argmax(y_prob, axis=-1)\n",
    "        err_valid[i, f] = error_rate(y_rest[val], y_pred)\n",
    "    #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
    "    NN = KNN(K=k)\n",
    "    NN = NN.fit(x_rest, y_rest)\n",
    "    y_prob, knns = NN.predict(x_test)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "    err_test[i]= error_rate(y_test, y_pred)\n",
    "\n",
    "plt.plot(k_list, err_test,  label='test')\n",
    "plt.errorbar(k_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('90-10 split')\n",
    "plt.show()\n",
    "\n",
    "n_test = num_instances// 10\n",
    "n_valid = num_instances// 10\n",
    "  # means train:valid:test = 8:1:1 (train using 80%)\n",
    "\n",
    "inds = np.random.permutation(num_instances)\n",
    "x_test, y_test = x[inds[:n_test]], y[inds[:n_test]]\n",
    "x_rest, y_rest = x[inds[n_test:]], y[inds[n_test:]]\n",
    "n_rest = num_instances - n_test\n",
    "\n",
    "#Plot the mean square error for different K values stored in K_list\n",
    "k_list = range(1,30)\n",
    "err_test, err_valid = np.zeros(len(k_list)), np.zeros((len(k_list), num_folds))\n",
    "for i, k in enumerate(k_list):\n",
    "    #Find the validation errors for num_folds splits for a given K\n",
    "    for f, (tr, val) in enumerate(cross_validate(n_rest, num_folds)):\n",
    "        NN = KNN(K=k)\n",
    "        NN = NN.fit(x_rest[tr], y_rest[tr])\n",
    "        y_prob, knns = NN.predict(x_rest[val])\n",
    "        y_pred = np.argmax(y_prob, axis=-1)\n",
    "        err_valid[i, f] = error_rate(y_rest[val], y_pred)\n",
    "    #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
    "    NN = KNN(K=k)\n",
    "    NN = NN.fit(x_rest, y_rest)\n",
    "    y_prob, knns = NN.predict(x_test)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "    err_test[i]= error_rate(y_test, y_pred)\n",
    "\n",
    "plt.plot(k_list, err_test,  label='test')\n",
    "plt.errorbar(k_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('80-20 split')\n",
    "plt.show()\n",
    "\n",
    "n_test = num_instances// 4\n",
    "n_valid = num_instances// 4\n",
    "  # means train:valid:test = 2:1:1 (train using 50%)\n",
    "\n",
    "inds = np.random.permutation(num_instances)\n",
    "x_test, y_test = x[inds[:n_test]], y[inds[:n_test]]\n",
    "x_rest, y_rest = x[inds[n_test:]], y[inds[n_test:]]\n",
    "n_rest = num_instances - n_test\n",
    "\n",
    "#Plot the mean square error for different K values stored in K_list\n",
    "k_list = range(1,5)\n",
    "err_test, err_valid = np.zeros(len(k_list)), np.zeros((len(k_list), num_folds))\n",
    "for i, k in enumerate(k_list):\n",
    "    #Find the validation errors for num_folds splits for a given K\n",
    "    for f, (tr, val) in enumerate(cross_validate(n_rest, num_folds)):\n",
    "        #print(f, (tr, val))\n",
    "        #print(y_rest[val], y_pred)\n",
    "        NN = KNN(K=k)\n",
    "        #print(x_rest[tr], y_rest[tr])\n",
    "        NN = NN.fit(x_rest[tr], y_rest[tr])\n",
    "        #print(x_rest[val])\n",
    "        y_prob, knns = NN.predict(x_rest[val])\n",
    "        print(y_prob)\n",
    "        y_pred = np.argmax(y_prob, axis=-1)\n",
    "        #print(y_prob, y_pred)\n",
    "        err_valid[i, f] = error_rate(y_rest[val], y_pred)\n",
    "    #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
    "    NN = KNN(K=k)\n",
    "    NN = NN.fit(x_rest, y_rest)\n",
    "    y_prob, knns = NN.predict(x_test)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "    #print(error_rate(y_test, y_pred))\n",
    "    err_test[i]= error_rate(y_test, y_pred)\n",
    "\n",
    "plt.plot(k_list, err_test,  label='test')\n",
    "plt.errorbar(k_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('50-50 split')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-60o8eLAVQtg"
   },
   "source": [
    "###Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRm2LMIUWq2n"
   },
   "source": [
    "####Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysSdWMdjen4W"
   },
   "outputs": [],
   "source": [
    "x = df1_np[:,:-1]\n",
    "y = df1_np[:,-1]\n",
    "\n",
    "(num_instances, num_features), num_classes = x.shape, len(np.unique(y, axis=0))\n",
    "inds = np.random.permutation(num_instances)\n",
    "\n",
    "#train-test split 50-50 split\n",
    "x_train, y_train = x[inds[:342]], y[inds[:342]]\n",
    "x_test, y_test = x[inds[342:]], y[inds[342:]]\n",
    "\n",
    "\n",
    "tree = DecisionTree(max_depth=20)\n",
    "probs_test = tree.fit(x_train, y_train).predict(x_test)\n",
    "y_pred = np.argmax(probs_test,1)\n",
    "accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
    "print(f'accuracy is {accuracy*100:.1f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaOafaaqphRg"
   },
   "source": [
    "####Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQQdKVJPpdjz"
   },
   "outputs": [],
   "source": [
    "#visualization (plot for 2 features -> most discriminant features)\n",
    "correct = y_test == y_pred\n",
    "incorrect = np.logical_not(correct)\n",
    "plt.scatter(x_train[:,0], x_train[:,1], c=y_train, marker='o', alpha=.2, label='train')\n",
    "plt.scatter(x_test[correct,0], x_test[correct,1], marker='.', c=y_pred[correct], label='correct')\n",
    "plt.scatter(x_test[incorrect,0], x_test[incorrect,1], marker='x', c=y_test[incorrect], label='misclassified')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW0DAKibVc5h"
   },
   "source": [
    "#### Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eL9y79JVeMN"
   },
   "outputs": [],
   "source": [
    "#decision boundary for DT\n",
    "#uses shape and size as data (because can only visualize 2 dimensions)\n",
    "\n",
    "x = df1_np[:, 1:3] # TA said to use 2 features for decision boundary plot\n",
    "y = df1_np[:,-1]\n",
    "(num_instances, num_features), num_classes = x.shape, len(np.unique(y, axis=0))\n",
    "inds = np.random.permutation(num_instances)\n",
    "\n",
    "#train-test split\n",
    "x_train, y_train = x[inds[:342]], y[inds[:342]]\n",
    "x_test, y_test = x[inds[342:]], y[inds[342:]]\n",
    "\n",
    "x0v = np.linspace(np.min(x[:,0]), np.max(x[:,0]),200)\n",
    "x1v = np.linspace(np.min(x[:,1]), np.max(x[:,1]),200)\n",
    "x0, x1 = np.meshgrid(x0v,x1v)\n",
    "x_all = np.vstack((x0.ravel(),x1.ravel())).T\n",
    "\n",
    "tree2 = DecisionTree(max_depth=200)\n",
    "y_train_prob = np.zeros((y_train.shape[0], 3))\n",
    "  # need to set to 3 because need 3 channels for RGBA\n",
    "y_train_prob[np.arange(y_train.shape[0]), y_train.astype(int)] = 1\n",
    "y_prob_all = tree2.fit(x_train, y_train).predict(x_all)\n",
    "b = np.zeros((y_prob_all.shape[0], 1))\n",
    "y_prob_all = np.concatenate((y_prob_all, b), axis=1)\n",
    "  # adds 3rd dimension for RGBA (filled with 0s)\n",
    "\n",
    "plt.scatter(x_train[:,0], x_train[:,1], c=y_train_prob, marker='o', alpha=1)\n",
    "plt.scatter(x_all[:,0], x_all[:,1], c=y_prob_all, marker='.', alpha=0.01)\n",
    "plt.ylabel('cell size')\n",
    "plt.xlabel('cell shape')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLHjfAnYMkM_"
   },
   "source": [
    "####Cross Validation for DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiB5OT2ZhBaj"
   },
   "source": [
    "We are using 5-fold CV, with 80% of the data used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHpOqos8Mmrc"
   },
   "outputs": [],
   "source": [
    "def error_rate(pred, label):\n",
    "  accuracy = np.sum(pred == label)/label.shape[0]\n",
    "  return 1-accuracy\n",
    "\n",
    "def cross_validate(n, n_folds=5):\n",
    "    #get the number of data samples in each split\n",
    "    n_val = n // n_folds\n",
    "    inds = np.random.permutation(n)\n",
    "    inds = []\n",
    "    for f in range(n_folds):\n",
    "        tr_inds = []\n",
    "        #get the validation indexes\n",
    "        val_inds = list(range(f * n_val, (f+1)*n_val))\n",
    "        #get the train indexes\n",
    "        if f > 0:\n",
    "            tr_inds = list(range(f*n_val))\n",
    "        if f < n_folds - 1:\n",
    "            tr_inds = tr_inds + list(range((f+1)*n_val, n))\n",
    "        #The yield statement suspends function’s execution and sends a value back to the caller\n",
    "        #but retains enough state information to enable function to resume where it is left off\n",
    "        yield tr_inds, val_inds\n",
    "\n",
    "num_folds = 10\n",
    "(num_instances, num_features), num_classes = x.shape, np.max(y)+1\n",
    "\n",
    "n_test = num_instances// 20\n",
    "n_valid = num_instances// 20\n",
    "  # means train:valid:test = 18:1:1 (train using 90%)\n",
    "\n",
    "\n",
    "inds = np.random.permutation(num_instances)\n",
    "x_test, y_test = x[inds[:n_test]], y[inds[:n_test]]\n",
    "x_rest, y_rest = x[inds[n_test:]], y[inds[n_test:]]\n",
    "n_rest = num_instances - n_test\n",
    "\n",
    "\n",
    "#Plot the mean square error for different K values stored in K_list\n",
    "depth_list = range(1,30)\n",
    "num_folds = 5\n",
    "err_test, err_valid = np.zeros(len(depth_list)), np.zeros((len(depth_list), num_folds))\n",
    "for i, depth in enumerate(depth_list):\n",
    "    #Find the validation errors for num_folds splits for a given K\n",
    "    for f, (tr, val) in enumerate(cross_validate(n_rest, num_folds)):\n",
    "        tree = DecisionTree(max_depth = depth)\n",
    "        tree = tree.fit(x_rest[tr], y_rest[tr])\n",
    "        err_valid[i, f] = error_rate(y_rest[val], np.argmax(tree.predict(x_rest[val]),1))\n",
    "    #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
    "    tree = DecisionTree(max_depth = depth)\n",
    "    tree = tree.fit(x_rest, y_rest)\n",
    "    err_test[i]= error_rate(y_test, np.argmax(tree.predict(x_test),1))\n",
    "    \n",
    "plt.plot(depth_list, err_test,  label='test')\n",
    "plt.errorbar(depth_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('90-10 split')\n",
    "plt.show()\n",
    "\n",
    "n_test = num_instances// 10\n",
    "n_valid = num_instances// 10\n",
    "  # means train:valid:test = 8:1:1 (train using 80%)\n",
    "\n",
    "inds = np.random.permutation(num_instances)\n",
    "x_test, y_test = x[inds[:n_test]], y[inds[:n_test]]\n",
    "x_rest, y_rest = x[inds[n_test:]], y[inds[n_test:]]\n",
    "n_rest = num_instances - n_test\n",
    "\n",
    "\n",
    "#Plot the mean square error for different K values stored in K_list\n",
    "depth_list = range(1,30)\n",
    "num_folds = 5\n",
    "err_test, err_valid = np.zeros(len(depth_list)), np.zeros((len(depth_list), num_folds))\n",
    "for i, depth in enumerate(depth_list):\n",
    "    #Find the validation errors for num_folds splits for a given K\n",
    "    for f, (tr, val) in enumerate(cross_validate(n_rest, num_folds)):\n",
    "        tree = DecisionTree(max_depth = depth)\n",
    "        tree = tree.fit(x_rest[tr], y_rest[tr])\n",
    "        err_valid[i, f] = error_rate(y_rest[val], np.argmax(tree.predict(x_rest[val]),1))\n",
    "    #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
    "    tree = DecisionTree(max_depth = depth)\n",
    "    tree = tree.fit(x_rest, y_rest)\n",
    "    err_test[i]= error_rate(y_test, np.argmax(tree.predict(x_test),1))\n",
    "    \n",
    "plt.plot(depth_list, err_test,  label='test')\n",
    "plt.errorbar(depth_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('80-20 split')\n",
    "plt.show()\n",
    "\n",
    "n_test = num_instances// 4\n",
    "n_valid = num_instances// 4\n",
    "  # means train:valid:test = 2:1:1 (train using 50%)\n",
    "\n",
    "\n",
    "inds = np.random.permutation(num_instances)\n",
    "x_test, y_test = x[inds[:n_test]], y[inds[:n_test]]\n",
    "x_rest, y_rest = x[inds[n_test:]], y[inds[n_test:]]\n",
    "n_rest = num_instances - n_test\n",
    "\n",
    "\n",
    "#Plot the mean square error for different K values stored in K_list\n",
    "depth_list = range(1,30)\n",
    "num_folds = 5\n",
    "err_test, err_valid = np.zeros(len(depth_list)), np.zeros((len(depth_list), num_folds))\n",
    "for i, depth in enumerate(depth_list):\n",
    "    #Find the validation errors for num_folds splits for a given K\n",
    "    for f, (tr, val) in enumerate(cross_validate(n_rest, num_folds)):\n",
    "        tree = DecisionTree(max_depth = depth)\n",
    "        tree = tree.fit(x_rest[tr], y_rest[tr])\n",
    "        err_valid[i, f] = error_rate(y_rest[val], np.argmax(tree.predict(x_rest[val]),1))\n",
    "    #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
    "    tree = DecisionTree(max_depth = depth)\n",
    "    tree = tree.fit(x_rest, y_rest)\n",
    "    err_test[i]= error_rate(y_test, np.argmax(tree.predict(x_test),1))\n",
    "    \n",
    "plt.plot(depth_list, err_test,  label='test')\n",
    "plt.errorbar(depth_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('50-50 split')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4BBtPF4pHN7"
   },
   "source": [
    "####Best Hyperparameters (Attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "claFFdrOpGcB"
   },
   "outputs": [],
   "source": [
    "# x = df1_np[:,:-1]\n",
    "# y = df1_np[:,-1]\n",
    "# #x, y = df1_np['data'][:,2:4], df1_np['target'][:,10]\n",
    "# (num_instances, num_features), num_classes = x.shape, np.max(y)+1\n",
    "# inds = np.random.permutation(num_instances)\n",
    "\n",
    "# #train-test split)\n",
    "# #x_train, y_train = x[inds[:342]], y[inds[:342]]\n",
    "# #x_test, y_test = x[inds[342:]], y[inds[342:]]\n",
    "\n",
    "# x_train, y_train = x[inds[:544]], y[inds[:544]]\n",
    "# x_test, y_test = x[inds[544:]], y[inds[544:]]\n",
    "\n",
    "# plot_training = []\n",
    "# plot_y = []\n",
    "\n",
    "# for i in range(1,20):\n",
    "#     for j in range(18):\n",
    "#         tree = DecisionTree(max_depth= i,min_leaf_instances=j)\n",
    "#         probs_test = tree.fit(x_train, y_train).predict(x_test)\n",
    "#         y_pred = np.argmax(probs_test,1)\n",
    "#         accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
    "#         plot_training.append(accuracy)\n",
    "#         plot_y.append(i)\n",
    "#         # print(f'accuracy is {accuracy*100:.1f}, i:{i}, j:{j}')\n",
    "\n",
    "# plt.plot(plot_y,plot_training)\n",
    "# maximum_accuracy = max(plot_training)\n",
    "# print(\"Maximum accuracy = \",maximum_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EWw8la2Dpz6"
   },
   "source": [
    "# DATASET 2: Hepatitis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6NtbzlXmOwg"
   },
   "source": [
    "##Task 1: Acquire, preprocess, and analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xpm1WEkJT7Qo"
   },
   "source": [
    "###Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuHLXwMyDvuZ"
   },
   "source": [
    "Opening file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apXJBWAMDrnQ"
   },
   "outputs": [],
   "source": [
    "#unhash the following snippet if you want to work straight from the csv file on google drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "df2 = pd.read_csv('gdrive/MyDrive/Colab Notebooks/Assignment1/hepatitis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rTOIGGyErq4"
   },
   "source": [
    "Remove Rows with Missing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u_Ss7SGEzE5"
   },
   "outputs": [],
   "source": [
    "print(df2.shape[0], 'initial rows')\n",
    "df2 = df2.replace('?', np.nan)\n",
    "df2 = df2.dropna()\n",
    "df2.reset_index(drop=True, inplace=True) #reset indices (so first instance at index 0, next at index 1...)\n",
    "df2 = df2.astype(float) # all values are float (instead of string)\n",
    "print(df2.shape[0], ' rows after remove instances with missing data ')\n",
    "\n",
    "df2['Class'] = df2['Class'].replace([1.0], 0)\n",
    "df2['Class'] = df2['Class'].replace([2.0], 1)\n",
    "  # replace 1 with 0, 2 with 1 (for decision boundary later)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qNKd-ovFbCb"
   },
   "source": [
    "Removal of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3UE8Gs-FcDQ"
   },
   "outputs": [],
   "source": [
    "#only removes outliers in continuous features (NOT categorical)\n",
    "df_cat = df2.loc[:, ['Class', 'SEX', 'STEROID', 'ANTIVIRALS', 'FATIGUE', 'MALAISE', 'ANOREXIA', 'LIVER_BIG', 'LIVER_FIRM', 'SPLEEN_PALPABLE', 'SPIDERS', 'ASCITES', 'VARICES', 'HISTOLOGY']]\n",
    "df_cont = df2.loc[:, ['AGE', 'BILIRUBIN', 'ALK_PHOSPHATE', 'SGOT', 'ALBUMIN', 'PROTIME']]\n",
    "idx = np.all(stats.zscore(df_cont) < 3, axis=1)\n",
    "df2 = pd.concat([df_cat.loc[idx], df_cont.loc[idx]], axis=1)\n",
    "df2 = df2.dropna()\n",
    "df2.reset_index(drop=True, inplace=True) #reset indices (so first instance at index 0, next at index 1...)\n",
    "\n",
    "print(df2.shape[0], ' rows after outlier removal')\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrsRcySqT-fL"
   },
   "source": [
    "###Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlmZ2otw6xPI"
   },
   "outputs": [],
   "source": [
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkycbwA7nRPA"
   },
   "source": [
    "Number of Occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUk0sWYNnSZO"
   },
   "outputs": [],
   "source": [
    "# 0 = die, 1 = live\n",
    "die = df2.loc[(df2['Class'] == 0)] \n",
    "num_die = die.shape[0]\n",
    "live = df2.loc[(df2['Class'] == 1)] \n",
    "num_live = live.shape[0]\n",
    "\n",
    "print('Total # Patients: ', num_live+num_die)\n",
    "print('# Live: ', num_live)\n",
    "print('# Die: ', num_die)\n",
    "per_die = (num_die)/(num_live+num_die) * 100\n",
    "print(\"% Die: \" + str(int(per_die)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSS6fYhg4UUH"
   },
   "source": [
    "####Distribution by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9w7iaMS4Xsw"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['AGE'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6EGa7fN48A-"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['SEX'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36JlkouJ4-Qp"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['STEROID'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7HqMixX5B0O"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['ANTIVIRALS'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFmDylfR5Eof"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['FATIGUE'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnyuONTN5G2E"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['MALAISE'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEo8sFZ45ItZ"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['ANOREXIA'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbGQDtaj5KTp"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['LIVER_BIG'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUJnunDx7U6f"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['LIVER_FIRM'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ex9Xt6e7jl2"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['SPLEEN_PALPABLE'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDMAZhJz7lJQ"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['SPIDERS'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgqfpdIi7nFJ"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['ASCITES'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fMd40417oDE"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['VARICES'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKQehH4X7pYx"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['BILIRUBIN'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zP5UR-Vc7qx7"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['ALK_PHOSPHATE'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWYQZy9d7sJD"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['SGOT'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "356xPTyg7ta-"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['ALBUMIN'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hYpVbKS7up0"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['PROTIME'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAFp4_Ys7v08"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df2['HISTOLOGY'], bins = 20, kde = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrgBJABB2ZJI"
   },
   "source": [
    "####Positive vs. Negative Distribution by Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VeZ85a-2ekX"
   },
   "outputs": [],
   "source": [
    "pos = df2[df2['Class'] == 1]\n",
    "neg = df2[df2['Class'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsUgvF0N2pyN"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['AGE'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['AGE'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOMUwY1Q3Mpk"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['SEX'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['SEX'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEmNkwn03XBQ"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['STEROID'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['STEROID'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fge6n-Ib3bi0"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['ANTIVIRALS'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['ANTIVIRALS'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gn8Zu2Z63itx"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['FATIGUE'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['FATIGUE'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzpU8A163qx5"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['MALAISE'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['MALAISE'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eC89v9uW3qdv"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['ANOREXIA'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['ANOREXIA'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_D5frBWt4Ez4"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['LIVER_BIG'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['LIVER_BIG'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqxF7EGv4JPT"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['LIVER_FIRM'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['LIVER_FIRM'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_t1I54f8gOr"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['SPLEEN_PALPABLE'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['SPLEEN_PALPABLE'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAn9tokS8mfG"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['SPIDERS'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['SPIDERS'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxyJSQjN8oG_"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['ASCITES'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['ASCITES'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soDFOlVb8peb"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['VARICES'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['VARICES'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKPnfIvI8q2a"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['BILIRUBIN'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['BILIRUBIN'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XS8N3ORK8sbg"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['ALK_PHOSPHATE'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['ALK_PHOSPHATE'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gI3aUgV8t9k"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['SGOT'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['SGOT'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bo-gRY1A8vX0"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['ALBUMIN'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['ALBUMIN'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BmXKwhN8wst"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['PROTIME'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['PROTIME'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwKkoI1y8yB7"
   },
   "outputs": [],
   "source": [
    "sns.histplot(pos['HISTOLOGY'], bins = 10, color = 'b', kde = False)\n",
    "sns.histplot(neg['HISTOLOGY'], bins = 10, color = 'r', kde = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4QSygdYFg5W"
   },
   "source": [
    "###Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-Pk2yxQFjM2"
   },
   "outputs": [],
   "source": [
    "corr_matrix = df2.corr()\n",
    "plt.clf()\n",
    "sns.heatmap(corr_matrix) #annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hd2dAZY7mdJm"
   },
   "source": [
    "##Task 2 & 3: Implement the Models and Run Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRsv0iryafew"
   },
   "source": [
    "###Convert to Numpy Array for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSPKkQGdaggB"
   },
   "outputs": [],
   "source": [
    "df2_np = df2.to_numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oxsz6wQnZ4mZ"
   },
   "source": [
    "###KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLpq4qBLaWT8"
   },
   "source": [
    "####Accuracy with 50-50 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlIAWSyNaRa-"
   },
   "outputs": [],
   "source": [
    "x = df2_np[:,1:] # TA said to train on all features initially for part 2 \n",
    "y = df2_np[:,0]\n",
    "\n",
    "\n",
    "(N, D), C = x.shape, np.max(y)+1\n",
    "print(f'instances (N) \\t {N} \\n features (D) \\t {D} \\n classes (C) \\t {C}')\n",
    "\n",
    "inds = np.random.permutation(N)\n",
    "\n",
    "x_train, y_train = x[inds[:40]], y[inds[:40]]\n",
    "x_test, y_test = x[inds[40:]], y[inds[40:]]\n",
    "\n",
    "model = KNN(K=3)\n",
    "\n",
    "y_prob, knns = model.fit(x_train, y_train).predict(x_test)\n",
    "print('knns shape:', knns.shape)\n",
    "print('y_prob shape:', y_prob.shape)\n",
    "\n",
    "#To get hard predictions by choosing the class with the maximum probability\n",
    "y_pred = np.argmax(y_prob,axis=-1)\n",
    "\n",
    "accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
    "\n",
    "print(f'accuracy is {accuracy*100:.1f}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WeT3T-68dPb5"
   },
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1eeuOY_dQwp"
   },
   "outputs": [],
   "source": [
    "#visualization (plot for 2 features -> most discriminant features)\n",
    "correct = y_test == y_pred\n",
    "incorrect = np.logical_not(correct)\n",
    "plt.scatter(x_train[:,-5], x_train[:,-2], c=y_train, marker='o', alpha=.2, label='train')\n",
    "plt.scatter(x_test[correct,-5], x_test[correct,-2], marker='.', c=y_pred[correct], label='correct')\n",
    "plt.scatter(x_test[incorrect,-5], x_test[incorrect,-2], marker='x', c=y_test[incorrect], label='misclassified')\n",
    "plt.legend()\n",
    "plt.ylabel('Albumin')\n",
    "plt.xlabel('Bilirubin')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFybTUcld9Hq"
   },
   "source": [
    "####Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZWYOlrgd_c2"
   },
   "outputs": [],
   "source": [
    "#decision boundary for KNN\n",
    "#uses shape and size as data (because can only visualize 2 dimensions)\n",
    "\n",
    "x = df2_np[:, [15,18]]\n",
    "y = df2_np[:,0]\n",
    "(num_instances, num_features), num_classes = x.shape, len(np.unique(y, axis=0))\n",
    "inds = np.random.permutation(num_instances)\n",
    "\n",
    "#train-test split\n",
    "x_train, y_train = x[inds[:40]], y[inds[:40]]\n",
    "x_test, y_test = x[inds[40:]], y[inds[40:]]\n",
    "\n",
    "x0v = np.linspace(np.min(x[:,0]), np.max(x[:,0]),200)\n",
    "x1v = np.linspace(np.min(x[:,1]), np.max(x[:,1]),200)\n",
    "x0, x1 = np.meshgrid(x0v,x1v)\n",
    "x_all = np.vstack((x0.ravel(),x1.ravel())).T\n",
    "\n",
    "model2H = KNN(K=3)\n",
    "y_train_prob = np.zeros((y_train.shape[0], 3))\n",
    "  # need to set to 3 because need 3 channels for RGBA\n",
    "y_train_prob[np.arange(y_train.shape[0]), y_train.astype(int)] = 1\n",
    "y_prob_all, _ = model2H.fit(x_train, y_train).predict(x_all)\n",
    "b = np.zeros((y_prob_all.shape[0], 1))\n",
    "y_prob_all = np.concatenate((y_prob_all, b), axis=1)\n",
    "  # adds 3rd dimension for RGBA (filled with 0s)\n",
    "\n",
    "y_pred_all = np.zeros_like(y_prob_all)\n",
    "y_pred_all[np.arange(x_all.shape[0]), np.argmax(y_prob_all, axis=-1)] = 1\n",
    "\n",
    "plt.scatter(x_train[:,0], x_train[:,1], c=y_train_prob, marker='o', alpha=1)\n",
    "plt.scatter(x_all[:,0], x_all[:,1], c=y_pred_all, marker='.', alpha=0.01)\n",
    "plt.ylabel('Albumin')\n",
    "plt.xlabel('Bilirubin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMX4tMlrMdi1"
   },
   "source": [
    "####Cross Validation for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtLQCsD8MffS"
   },
   "outputs": [],
   "source": [
    "def error_rate(pred, label):\n",
    "  accuracy = np.sum(pred == label)/label.shape[0]\n",
    "  return 1-accuracy\n",
    "\n",
    "def cross_validate(n, n_folds=10):\n",
    "    #get the number of data samples in each split\n",
    "    n_val = n // n_folds\n",
    "    inds = np.random.permutation(n)\n",
    "    inds = []\n",
    "    for f in range(n_folds):\n",
    "        tr_inds = []\n",
    "        #get the validation indexes\n",
    "        val_inds = list(range(f * n_val, (f+1)*n_val))\n",
    "        #get the train indexes\n",
    "        if f > 0:\n",
    "            tr_inds = list(range(f*n_val))\n",
    "        if f < n_folds - 1:\n",
    "            tr_inds = tr_inds + list(range((f+1)*n_val, n))\n",
    "        #The yield statement suspends function’s execution and sends a value back to the caller\n",
    "        #but retains enough state information to enable function to resume where it is left off\n",
    "        yield tr_inds, val_inds\n",
    "\n",
    "\n",
    "num_folds = 10\n",
    "(num_instances, num_features), num_classes = x.shape, np.max(y)+1\n",
    "\n",
    "n_test = num_instances// 20\n",
    "n_valid = num_instances// 20\n",
    "  # means train:valid:test = 18:1:1 (train using 90%)\n",
    "\n",
    "inds = np.random.permutation(num_instances)\n",
    "x_test, y_test = x[inds[:n_test]], y[inds[:n_test]]\n",
    "x_rest, y_rest = x[inds[n_test:]], y[inds[n_test:]]\n",
    "n_rest = num_instances - n_test\n",
    "\n",
    "#Plot the mean square error for different K values stored in K_list\n",
    "k_list = range(1,30)\n",
    "err_test, err_valid = np.zeros(len(k_list)), np.zeros((len(k_list), num_folds))\n",
    "for i, k in enumerate(k_list):\n",
    "    #Find the validation errors for num_folds splits for a given K\n",
    "    for f, (tr, val) in enumerate(cross_validate(n_rest, num_folds)):\n",
    "        NN = KNN(K=k)\n",
    "        NN = NN.fit(x_rest[tr], y_rest[tr])\n",
    "        y_prob, knns = NN.predict(x_rest[val])\n",
    "        y_pred = np.argmax(y_prob, axis=-1)\n",
    "        err_valid[i, f] = error_rate(y_rest[val], y_pred)\n",
    "    #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
    "    NN = KNN(K=k)\n",
    "    NN = NN.fit(x_rest, y_rest)\n",
    "    y_prob, knns = NN.predict(x_test)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "    err_test[i]= error_rate(y_test, y_pred)\n",
    "\n",
    "plt.plot(k_list, err_test,  label='test')\n",
    "plt.errorbar(k_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('90-10 split')\n",
    "plt.show()\n",
    "\n",
    "n_test = num_instances// 10\n",
    "n_valid = num_instances// 10\n",
    "  # means train:valid:test = 8:1:1 (train using 80%)\n",
    "\n",
    "inds = np.random.permutation(num_instances)\n",
    "x_test, y_test = x[inds[:n_test]], y[inds[:n_test]]\n",
    "x_rest, y_rest = x[inds[n_test:]], y[inds[n_test:]]\n",
    "n_rest = num_instances - n_test\n",
    "\n",
    "#Plot the mean square error for different K values stored in K_list\n",
    "k_list = range(1,30)\n",
    "err_test, err_valid = np.zeros(len(k_list)), np.zeros((len(k_list), num_folds))\n",
    "for i, k in enumerate(k_list):\n",
    "    #Find the validation errors for num_folds splits for a given K\n",
    "    for f, (tr, val) in enumerate(cross_validate(n_rest, num_folds)):\n",
    "        NN = KNN(K=k)\n",
    "        NN = NN.fit(x_rest[tr], y_rest[tr])\n",
    "        y_prob, knns = NN.predict(x_rest[val])\n",
    "        y_pred = np.argmax(y_prob, axis=-1)\n",
    "        err_valid[i, f] = error_rate(y_rest[val], y_pred)\n",
    "    #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
    "    NN = KNN(K=k)\n",
    "    NN = NN.fit(x_rest, y_rest)\n",
    "    y_prob, knns = NN.predict(x_test)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "    err_test[i]= error_rate(y_test, y_pred)\n",
    "\n",
    "plt.plot(k_list, err_test,  label='test')\n",
    "plt.errorbar(k_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('80-20 split')\n",
    "plt.show()\n",
    "\n",
    "n_test = num_instances// 4\n",
    "n_valid = num_instances// 4\n",
    "  # means train:valid:test = 2:1:1 (train using 50%)\n",
    "\n",
    "inds = np.random.permutation(num_instances)\n",
    "x_test, y_test = x[inds[:n_test]], y[inds[:n_test]]\n",
    "x_rest, y_rest = x[inds[n_test:]], y[inds[n_test:]]\n",
    "n_rest = num_instances - n_test\n",
    "\n",
    "#Plot the mean square error for different K values stored in K_list\n",
    "k_list = range(1,30)\n",
    "err_test, err_valid = np.zeros(len(k_list)), np.zeros((len(k_list), num_folds))\n",
    "for i, k in enumerate(k_list):\n",
    "    #Find the validation errors for num_folds splits for a given K\n",
    "    for f, (tr, val) in enumerate(cross_validate(n_rest, num_folds)):\n",
    "        NN = KNN(K=k)\n",
    "        NN = NN.fit(x_rest[tr], y_rest[tr])\n",
    "        y_prob, knns = NN.predict(x_rest[val])\n",
    "        y_pred = np.argmax(y_prob, axis=-1)\n",
    "        err_valid[i, f] = error_rate(y_rest[val], y_pred)\n",
    "    #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
    "    NN = KNN(K=k)\n",
    "    NN = NN.fit(x_rest, y_rest)\n",
    "    y_prob, knns = NN.predict(x_test)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "    err_test[i]= error_rate(y_test, y_pred)\n",
    "\n",
    "plt.plot(k_list, err_test,  label='test')\n",
    "plt.errorbar(k_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('50-50 split')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTFg9OYFgUFb"
   },
   "source": [
    "###Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vJeX9gjgWPg"
   },
   "source": [
    "####Accuracy with 50-50 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K5j_qDwgYUH"
   },
   "outputs": [],
   "source": [
    "x = df2_np[:,1:] # TA said to train on all features initially for part 2 \n",
    "y = df2_np[:,0]\n",
    "\n",
    "(N, D), C = x.shape, np.max(y)+1\n",
    "print(f'instances (N) \\t {N} \\n features (D) \\t {D} \\n classes (C) \\t {C}')\n",
    "\n",
    "inds = np.random.permutation(N)\n",
    "\n",
    "x_train, y_train = x[inds[:40]], y[inds[:40]]\n",
    "x_test, y_test = x[inds[40:]], y[inds[40:]]\n",
    "\n",
    "tree = DecisionTree(max_depth=20)\n",
    "tree = tree.fit(x_train, y_train)\n",
    "probs_test = tree.predict(x_test)\n",
    "y_pred = np.argmax(probs_test,1)\n",
    "accuracy = np.sum(y_pred == y_test)/y_test.shape[0]\n",
    "print(f'accuracy is {accuracy*100:.1f}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkzxAklVglqz"
   },
   "source": [
    "####Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFh9kkMygnct"
   },
   "outputs": [],
   "source": [
    "#visualization (plot for 2 features -> most discriminant features)\n",
    "correct = y_test == y_pred\n",
    "incorrect = np.logical_not(correct)\n",
    "plt.scatter(x_train[:,-5], x_train[:,-2], c=y_train, marker='o', alpha=.2, label='train')\n",
    "plt.scatter(x_test[correct,-5], x_test[correct,-2], marker='.', c=y_pred[correct], label='correct')\n",
    "plt.scatter(x_test[incorrect,-5], x_test[incorrect,-2], marker='x', c=y_test[incorrect], label='misclassified')\n",
    "plt.legend()\n",
    "plt.ylabel('Albumin')\n",
    "plt.xlabel('Bilirubin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PZJ3RcwgtRr"
   },
   "source": [
    "####Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzSbP20jgwzr"
   },
   "outputs": [],
   "source": [
    "#decision boundary for DT\n",
    "x = df2_np[:, [15,18]]\n",
    "y = df2_np[:,0]\n",
    "\n",
    "(num_instances, num_features), num_classes = x.shape, len(np.unique(y, axis=0))\n",
    "inds = np.random.permutation(num_instances)\n",
    "\n",
    "x_train, y_train = x[inds[:40]], y[inds[:40]]\n",
    "x_test, y_test = x[inds[40:]], y[inds[40:]]\n",
    "\n",
    "x0v = np.linspace(np.min(x[:,0]), np.max(x[:,0]),200)\n",
    "x1v = np.linspace(np.min(x[:,1]), np.max(x[:,1]),200)\n",
    "x0, x1 = np.meshgrid(x0v,x1v)\n",
    "x_all = np.vstack((x0.ravel(),x1.ravel())).T\n",
    "\n",
    "tree2 = DecisionTree(max_depth=20)\n",
    "y_train_prob = np.zeros((y_train.shape[0], 3))\n",
    "  # need to set to 3 because need 3 channels for RGBA\n",
    "y_train_prob[np.arange(y_train.shape[0]), y_train.astype(int)] = 1\n",
    "y_prob_all = tree2.fit(x_train, y_train).predict(x_all)\n",
    "b = np.zeros((y_prob_all.shape[0], 1))\n",
    "y_prob_all = np.concatenate((y_prob_all, b), axis=1)\n",
    "  # adds 3rd dimension for RGBA (filled with 0s)\n",
    "\n",
    "plt.scatter(x_train[:,0], x_train[:,1], c=y_train_prob, marker='o', alpha=1)\n",
    "plt.scatter(x_all[:,0], x_all[:,1], c=y_prob_all, marker='.', alpha=0.01)\n",
    "plt.ylabel('Bilirubin')\n",
    "plt.xlabel('Albumin')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAUfR73E_C3G"
   },
   "source": [
    "####Cross Validation for DT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsl1Mu1U_M_E"
   },
   "source": [
    "We are using 5-fold CV with 2:1:1 train:validation:test (50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUwWvOHX_HXU"
   },
   "outputs": [],
   "source": [
    "def error_rate(pred, label):\n",
    "  accuracy = np.sum(pred == label)/label.shape[0]\n",
    "  return 1-accuracy\n",
    "\n",
    "def cross_validate(n, n_folds=5):\n",
    "    #get the number of data samples in each split\n",
    "    n_val = n // n_folds\n",
    "    inds = np.random.permutation(n)\n",
    "    inds = []\n",
    "    for f in range(n_folds):\n",
    "        tr_inds = []\n",
    "        #get the validation indexes\n",
    "        val_inds = list(range(f * n_val, (f+1)*n_val))\n",
    "        #get the train indexes\n",
    "        if f > 0:\n",
    "            tr_inds = list(range(f*n_val))\n",
    "        if f < n_folds - 1:\n",
    "            tr_inds = tr_inds + list(range((f+1)*n_val, n))\n",
    "        #The yield statement suspends function’s execution and sends a value back to the caller\n",
    "        #but retains enough state information to enable function to resume where it is left off\n",
    "        yield tr_inds, val_inds\n",
    "\n",
    "num_folds = 10\n",
    "(num_instances, num_features), num_classes = x.shape, np.max(y)+1\n",
    "\n",
    "n_test = num_instances// 20\n",
    "n_valid = num_instances// 20\n",
    "  # means train:valid:test = 18:1:1 (train using 90%)\n",
    "\n",
    "inds = np.random.permutation(num_instances)\n",
    "x_test, y_test = x[inds[:n_test]], y[inds[:n_test]]\n",
    "x_rest, y_rest = x[inds[n_test:]], y[inds[n_test:]]\n",
    "n_rest = num_instances - n_test\n",
    "\n",
    "\n",
    "#Plot the mean square error for different K values stored in K_list\n",
    "depth_list = range(1,30)\n",
    "num_folds = 5\n",
    "err_test, err_valid = np.zeros(len(depth_list)), np.zeros((len(depth_list), num_folds))\n",
    "for i, depth in enumerate(depth_list):\n",
    "    #Find the validation errors for num_folds splits for a given K\n",
    "    for f, (tr, val) in enumerate(cross_validate(n_rest, num_folds)):\n",
    "        tree = DecisionTree(max_depth = depth)\n",
    "        tree = tree.fit(x_rest[tr], y_rest[tr])\n",
    "        err_valid[i, f] = error_rate(y_rest[val], np.argmax(tree.predict(x_rest[val]),1))\n",
    "    #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
    "    tree = DecisionTree(max_depth = depth)\n",
    "    tree = tree.fit(x_rest, y_rest)\n",
    "    err_test[i]= error_rate(y_test, np.argmax(tree.predict(x_test),1))\n",
    "    \n",
    "plt.plot(depth_list, err_test,  label='test')\n",
    "plt.errorbar(depth_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('90-10 split')\n",
    "plt.show()\n",
    "\n",
    "n_test = num_instances// 10\n",
    "n_valid = num_instances// 10\n",
    "  # means train:valid:test = 8:1:1 (train using 80%)\n",
    "\n",
    "inds = np.random.permutation(num_instances)\n",
    "x_test, y_test = x[inds[:n_test]], y[inds[:n_test]]\n",
    "x_rest, y_rest = x[inds[n_test:]], y[inds[n_test:]]\n",
    "n_rest = num_instances - n_test\n",
    "\n",
    "\n",
    "#Plot the mean square error for different K values stored in K_list\n",
    "depth_list = range(1,30)\n",
    "num_folds = 5\n",
    "err_test, err_valid = np.zeros(len(depth_list)), np.zeros((len(depth_list), num_folds))\n",
    "for i, depth in enumerate(depth_list):\n",
    "    #Find the validation errors for num_folds splits for a given K\n",
    "    for f, (tr, val) in enumerate(cross_validate(n_rest, num_folds)):\n",
    "        tree = DecisionTree(max_depth = depth)\n",
    "        tree = tree.fit(x_rest[tr], y_rest[tr])\n",
    "        err_valid[i, f] = error_rate(y_rest[val], np.argmax(tree.predict(x_rest[val]),1))\n",
    "    #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
    "    tree = DecisionTree(max_depth = depth)\n",
    "    tree = tree.fit(x_rest, y_rest)\n",
    "    err_test[i]= error_rate(y_test, np.argmax(tree.predict(x_test),1))\n",
    "    \n",
    "plt.plot(depth_list, err_test,  label='test')\n",
    "plt.errorbar(depth_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('80-20 split')\n",
    "plt.show()\n",
    "\n",
    "n_test = num_instances// 4\n",
    "n_valid = num_instances// 4\n",
    "  # means train:valid:test = 2:1:1 (train using 50%)\n",
    "\n",
    "\n",
    "inds = np.random.permutation(num_instances)\n",
    "x_test, y_test = x[inds[:n_test]], y[inds[:n_test]]\n",
    "x_rest, y_rest = x[inds[n_test:]], y[inds[n_test:]]\n",
    "n_rest = num_instances - n_test\n",
    "\n",
    "\n",
    "#Plot the mean square error for different K values stored in K_list\n",
    "depth_list = range(1,30)\n",
    "num_folds = 5\n",
    "err_test, err_valid = np.zeros(len(depth_list)), np.zeros((len(depth_list), num_folds))\n",
    "for i, depth in enumerate(depth_list):\n",
    "    #Find the validation errors for num_folds splits for a given K\n",
    "    for f, (tr, val) in enumerate(cross_validate(n_rest, num_folds)):\n",
    "        tree = DecisionTree(max_depth = depth)\n",
    "        tree = tree.fit(x_rest[tr], y_rest[tr])\n",
    "        err_valid[i, f] = error_rate(y_rest[val], np.argmax(tree.predict(x_rest[val]),1))\n",
    "    #this is the part that we don't do in a usual setup. We don't touch the test set until the very end. \n",
    "    tree = DecisionTree(max_depth = depth)\n",
    "    tree = tree.fit(x_rest, y_rest)\n",
    "    err_test[i]= error_rate(y_test, np.argmax(tree.predict(x_test),1))\n",
    "    \n",
    "plt.plot(depth_list, err_test,  label='test')\n",
    "plt.errorbar(depth_list, np.mean(err_valid, axis=1), np.std(err_valid, axis=1), label='validation')\n",
    "plt.legend()\n",
    "plt.xlabel('Tree depth')\n",
    "plt.ylabel('error rate')\n",
    "plt.title('50-50 split')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MCHl00W8TMnG",
    "rhvAzOx8LXht",
    "ZnkX4OHWS2MN",
    "FxRN-5mbz7U1",
    "_p5jguInUHR4",
    "3BhJadnn1D_Z",
    "iePsZdarWMmd",
    "e44GbzguWnkQ",
    "NRm2LMIUWq2n",
    "kaOafaaqphRg",
    "vLHjfAnYMkM_",
    "WSS6fYhg4UUH",
    "XrgBJABB2ZJI",
    "T4QSygdYFg5W",
    "uLpq4qBLaWT8",
    "IkzxAklVglqz"
   ],
   "name": "Project #1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
